{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initial config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import bisect\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import os\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "from nltk.tree import *\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import nltk.stem.snowball\n",
    "import string\n",
    "from nltk.draw.tree import TreeView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "public='/home/users2/mehrotsh/scripts/packages/stanford-corenlp-full-2018-02-27/'\n",
    "personal='/home/samarth/stanford-corenlp-full-2018-02-27/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(public)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(): \n",
    "    return defaultdict(tree)\n",
    "\n",
    "\n",
    "def _leadingSpaces_(target):\n",
    "    return len(target) - len(target.lstrip())\n",
    "\n",
    "def _findParent_(curIndent, parid, treeRef):\n",
    "    tmpid = parid\n",
    "    while (curIndent <= treeRef[tmpid]['indent']):\n",
    "        tmpid = treeRef[tmpid]['parid']\n",
    "    return tmpid\n",
    "\n",
    "\n",
    "def generateTree(rawTokens, treeRef):\n",
    "\n",
    "    # (token\n",
    "    REGEX_OPEN = r\"^\\s*\\(([a-zA-Z0-9_']*)\\s*$\"\n",
    "    # (token (tok1 tok2) (tok3 tok4) .... (tokx toky))\n",
    "    REGEX_COMP = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*((?:[(]([a-zA-Z0-9_;.,?'!]+)\\s*([a-zA-Z0-9_;\\.,?!']+)[)]\\s*)+)\"    \n",
    "    # (, ,) as stand-alone. Used for match() not search()\n",
    "    REGEX_PUNC = r\"^\\s*\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "    # (tok1 tok2) as stand-alone\n",
    "    REGEX_SOLO_PAIR = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*([a-zA-Z0-9_']+)\\)\"\n",
    "    # (tok1 tok2) used in search()\n",
    "    REGEX_ISOL_IN_COMP = r\"\\(([a-zA-Z0-9_;.,?!']+)\\s*([a-zA-Z0-9_;.,?!']+)\\)\"\n",
    "    # (punc punc) used in search()\n",
    "    REGEX_PUNC_SOLO = r\"\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "   \n",
    "    treeRef[len(treeRef)] = {'curid':0, \n",
    "                             'parid':-1, \n",
    "                             'posOrTok':'ROOT', \n",
    "                             'indent':0,\n",
    "                            'children':[],\n",
    "                            'childrenTok':[]}\n",
    "    ID_CTR = 1\n",
    "    \n",
    "    for tok in rawTokens[1:]:\n",
    "        \n",
    "        curIndent = _leadingSpaces_(tok) \n",
    "        parid = _findParent_(curIndent, ID_CTR-1, treeRef)\n",
    "        \n",
    "        # CHECK FOR COMPOSITE TOKENS\n",
    "        checkChild = re.match(REGEX_COMP, tok)\n",
    "        if (checkChild):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkChild.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            upCTR = ID_CTR\n",
    "            ID_CTR += 1\n",
    "            \n",
    "            subCheck = re.sub(REGEX_PUNC_SOLO,'',checkChild.group(2))\n",
    "            subs = re.findall(REGEX_ISOL_IN_COMP, subCheck) \n",
    "            for ch in subs:\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':upCTR, \n",
    "                                   'posOrTok':ch[0], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':ID_CTR-1, \n",
    "                                   'posOrTok':ch[1], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "            continue\n",
    "           \n",
    "\n",
    "            \n",
    "        checkSingle = re.match(REGEX_SOLO_PAIR, tok)\n",
    "        if (checkSingle):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkSingle.group(1), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':ID_CTR-1, \n",
    "                               'posOrTok':checkSingle.group(2), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        checkPunc = re.match(REGEX_PUNC, tok)\n",
    "        if (checkPunc): # ignore punctuation\n",
    "            continue\n",
    "\n",
    "        checkMatch = re.match(REGEX_OPEN, tok)\n",
    "        if (checkMatch):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkMatch.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "\n",
    "    return\n",
    "            \n",
    "\n",
    "def flipTree(treeRef):\n",
    "    # Pass 1 fill in children\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            bisect.insort(treeRef[v['parid']]['children'], k)\n",
    "    # Pass 2 map children to tokens\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            treeRef[k]['childrenTok'] = [treeRef[ch]['posOrTok'] for ch in treeRef[k]['children']]\n",
    "    treeRef[0]['childrenTok'] = treeRef[1]['posOrTok']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isLeaf_(tree, parentNode):\n",
    "    return (len(tree[parentNode]['children']) == 0)\n",
    "\n",
    "def _isPreterminal_(tree, parentNode):\n",
    "    for idx in tree[parentNode]['children']:\n",
    "        if not _isLeaf_(tree, idx):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel\n",
    "'''\n",
    "\n",
    "def _cdHelper_(tree1, tree2, node1, node2, store, lam, SST_ON):\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "        # same children tokens\n",
    "        if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "            # Check if both nodes are pre-terminal\n",
    "            if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "                store[node1, node2] = lam\n",
    "                return\n",
    "            # Not pre-terminal. Recurse among the children of both token trees.\n",
    "            else:\n",
    "                nChildren = len(tree1[node1]['children'])\n",
    "\n",
    "                runningTotal = None\n",
    "                for idx in range(nChildren):\n",
    "                     # index ->  node_id\n",
    "                    tmp_n1 = tree1[node1]['children'][idx]\n",
    "                    tmp_n2 = tree2[node2]['children'][idx]\n",
    "                    # Recursively run helper\n",
    "                    _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON)\n",
    "                    # Set the initial value for the layer. Else multiplicative product.\n",
    "                    if (runningTotal == None):\n",
    "                        runningTotal = SST_ON + store[tmp_n1, tmp_n2]\n",
    "                    else:\n",
    "                        runningTotal *= (SST_ON + store[tmp_n1, tmp_n2])\n",
    "\n",
    "                store[node1, node2] = lam * runningTotal\n",
    "                return\n",
    "        else:\n",
    "            store[node1, node2] = 0\n",
    "    else: # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "\n",
    "def _cdKernel_(tree1, tree2, lam, SST_ON):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def CollinsDuffy(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON):\n",
    "    raw_score = _cdKernel_(tree1, tree2, lam, SST_ON)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _cdKernel_(tree1, tree1, lam, SST_ON)\n",
    "        t2_score = _cdKernel_(tree2, tree2, lam, SST_ON)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Implementation of the Partial Tree (PT) Kernel from:\n",
    "\"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees\"\n",
    "by Alessandro Moschitti\n",
    "'''\n",
    "\n",
    "'''\n",
    "The delta function is stolen from the Collins-Duffy kernel\n",
    "'''\n",
    "\n",
    "def _deltaP_(tree1, tree2, seq1, seq2, store, lam, mu, p):\n",
    "\n",
    "#     # Enumerate subsequences of length p+1 for each child set\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # generate delta(a,b)\n",
    "        _delta_(tree1, tree2, seq1[-1], seq2[-1], store, lam, mu)\n",
    "        if store[seq1[-1], seq2[-1]] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            runningTot = 0\n",
    "            for i in range(p-1, len(seq1)-1):\n",
    "                for r in range(p-1, len(seq2)-1):\n",
    "                    scaleFactor = pow(lam, len(seq1[:-1])-i+len(seq2[:-1])-r)\n",
    "                    dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-1)\n",
    "                    runningTot += (scaleFactor * dp)\n",
    "            return runningTot\n",
    "\n",
    "def _delta_(tree1, tree2, node1, node2, store, lam, mu):\n",
    "\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "\n",
    "        if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "            if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "                store[node1, node2] = lam\n",
    "            else:\n",
    "                store[node1, node2] = 0\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # establishes p_max\n",
    "            childmin = min(len(tree1[node1]['children']), len(tree2[node2]['children']))\n",
    "            deltaTot = 0\n",
    "            for p in range(1,childmin+1):\n",
    "                # compute delta_p\n",
    "                deltaTot += _deltaP_(tree1, tree2,\n",
    "                                     tree1[node1]['children'],\n",
    "                                     tree2[node2]['children'], store, lam, mu, p)\n",
    "\n",
    "            store[node1, node2] = mu * (pow(lam,2) + deltaTot)\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "def _ptKernel_(tree1, tree2, lam, mu):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _delta_(tree1, tree2, i, j, store, lam, mu)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def MoschittiPT(tree1, tree2, lam, mu, NORMALIZE_FLAG):\n",
    "    raw_score = _ptKernel_(tree1, tree2, lam, mu)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _ptKernel_(tree1, tree1, lam, mu)\n",
    "        t2_score = _ptKernel_(tree2, tree2, lam, mu)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNLPToks(rawSentence):\n",
    "    output = nlp.annotate(rawSentence, properties={'annotators': 'tokenize,ssplit,pos,parse','outputFormat': 'json','timeout':'50000'})\n",
    "    output=ast.literal_eval(output)\n",
    "    tokens = output['sentences'][0]['tokens']\n",
    "    parse = output['sentences'][0]['parse'].split(\"\\n\")\n",
    "    return {\n",
    "        'toks':tokens, 'parse':parse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuffyScore(sent1,sent2):\n",
    "    tree_1=tree()\n",
    "    tree_2=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    out2=getNLPToks(sent2)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    generateTree(out2['parse'],tree_2)\n",
    "    flipTree(tree_1)\n",
    "    flipTree(tree_2)\n",
    "    (rscore_st, nscore_st) = CollinsDuffy(tree_1, tree_2, 0.8, 1, 1)\n",
    "    return rscore_st,nscore_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoschittiScore(sent1,sent2):\n",
    "    tree_1=tree()\n",
    "    tree_2=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    out2=getNLPToks(sent2)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    generateTree(out2['parse'],tree_2)\n",
    "    flipTree(tree_1)\n",
    "    flipTree(tree_2)\n",
    "    (rscore_st, nscore_st) = MoschittiPT(tree_1, tree_2, 0.8, 1, 1)\n",
    "#     return rscore_st,nscore_st\n",
    "    return nscore_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing on Project Gutenberg samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating parse trees for the new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"./new/pierre.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>1, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "parseTrees=list()\n",
    "for sent in text:\n",
    "    print(i)\n",
    "    sentParse=getNLPToks(sent)\n",
    "    tempTree=tree()\n",
    "    generateTree(sentParse['parse'],tempTree)\n",
    "    flipTree(tempTree)\n",
    "    parseTrees.append(tempTree)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parseTrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading candidates and creating parse trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential=\"./potential/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentialParseTrees=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>1, candidate))\n",
    "    pTrees=list()\n",
    "    for sent in candidate:\n",
    "        sentParse=getNLPToks(sent)\n",
    "        tempTree=tree()\n",
    "        generateTree(sentParse['parse'],tempTree)\n",
    "        flipTree(tempTree)\n",
    "        pTrees.append(tempTree)\n",
    "    potentialParseTrees[file]=pTrees\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScores=list()\n",
    "i=0\n",
    "for tr in parseTrees:\n",
    "#     print(i)\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    sentScoreDict=dict()\n",
    "    for file in os.listdir(potential):\n",
    "#         print(file)\n",
    "        bookTrees=potentialParseTrees[file]\n",
    "        df=list()\n",
    "        for bTree in bookTrees:\n",
    "            (rscore_st, nscore_st) = MoschittiPT(tr, bTree, 0.8, 1, 1)\n",
    "            df.append(nscore_st)\n",
    "#         print(df)\n",
    "        sentScoreDict[file]=df\n",
    "    allScores.append(sentScoreDict)\n",
    "#     print('over')\n",
    "    i=i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScores=allScores[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books=dict()\n",
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>1, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60,70):\n",
    "    print('Sentence',i)\n",
    "    print('Original Sent',text[i])\n",
    "    for book in os.listdir(potential):\n",
    "        print(book)\n",
    "        maxIndex=allScores[i][book].index(max(allScores[i][book]))\n",
    "        print('Score',allScores[i][book][maxIndex])\n",
    "        print('Similar sentence:',books[book][maxIndex])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScores[600]['2.txt'].index(max(allScores[600]['2.txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allScores[0]['5.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(allScores)):\n",
    "    scoreTuple=(0,0,0,0)\n",
    "    for fl in os.listdir(potential):\n",
    "        scores=allScores[i][fl]\n",
    "        for j in range(len(scores)):\n",
    "            scoreTuples.append((i,fl,j,scores[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples.sort(key=lambda tup: tup[3],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scoreTuples[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing on Bible sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two related sentences - high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1='Behold, a virgin shall conceive and bear a son, and his name shall be called Emmanuel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2='behold, a virgin shall conceive in the womb, and shall bring forth a son, and thou shalt call his name Emmanuel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent1,sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two related sentences - high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3='And thou, Bethlehem, in the land of Juda, art not the least among the princes of Juda: for out of thee shall come a Governor, that shall rule my people Israel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent4='And thou, Bethleem, house of Ephratha, art few in number to be reckoned among the thousands of Juda; yet out of thee shall one come forth to me, to be a ruler of Israel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent3,sent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sentences that are not highly related, not such a high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent1,sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5='In Rama was there a voice heard, lamentation, and weeping, and great mourning, Rachel weeping for her children, and would not be comforted because they are not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent6='A voice was heard in Rama, of lamentation, and of weeping, and wailing; Rachel would not cease weeping for her children, because they are not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent5,sent6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent5,sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent7=' Then saith Jesus unto them, All ye shall be offended because of me this night: for it is written, I will smite the shepherd, and the sheep of the flock shall be scattered abroad.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent8='Awake, O sword, against my shepherds, and against the man who is my citizen, saith the Lord Almighty: smite the shepherds, and draw out the sheep: and I will bring mine hand upon the little ones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent7,sent8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very similar but still a reasonably high score (False positive), might be a parsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent7,sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent9='Jesus said unto him, Thou shalt love the Lord thy God with all thy heart, and with all thy soul, and with all thy mind.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent10='And thou shalt love the Lord thy God with all thy mind, and with all thy soul, and all thy strength'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent9,sent10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMoschittiScore(sent9,sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on chunks of the bible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential=\"./potential/\"\n",
    "booksList=os.listdir(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"./new/matthew\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "parseTrees=list()\n",
    "for sent in text:\n",
    "    print(i)\n",
    "    sentParse=getNLPToks(sent)\n",
    "    tempTree=tree()\n",
    "    generateTree(sentParse['parse'],tempTree)\n",
    "    flipTree(tempTree)\n",
    "    parseTrees.append(tempTree)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickling_on = open(\"./tempOutput/parseTrees.pickle\",\"wb\")\n",
    "pickle.dump(parseTrees, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential=\"./potential/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentialParseTrees=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>5, candidate))\n",
    "    pTrees=list()\n",
    "    for sent in candidate:\n",
    "        sentParse=getNLPToks(sent)\n",
    "        tempTree=tree()\n",
    "        generateTree(sentParse['parse'],tempTree)\n",
    "        flipTree(tempTree)\n",
    "        pTrees.append(tempTree)\n",
    "    potentialParseTrees[file]=pTrees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickling_on = open(\"./tempOutput/potentialParseTrees.pickle\",\"wb\")\n",
    "pickle.dump(potentialParseTrees, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScores=list()\n",
    "i=0\n",
    "for tr in parseTrees:\n",
    "#     print(i)\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    sentScoreDict=dict()\n",
    "    for file in os.listdir(potential):\n",
    "#         print(file)\n",
    "        bookTrees=potentialParseTrees[file]\n",
    "        df=list()\n",
    "        for bTree in bookTrees:\n",
    "            (rscore_st, nscore_st) = MoschittiPT(tr, bTree, 0.8, 1, 1)\n",
    "            df.append(nscore_st)\n",
    "#         print(df)\n",
    "        sentScoreDict[file]=df\n",
    "    allScores.append(sentScoreDict)\n",
    "#     print('over')\n",
    "    i=i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickling_on = open(\"./tempOutput/allScores.pickle\",\"wb\")\n",
    "pickle.dump(allScores, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"./tempOutput/allScores.pickle\",\"rb\")\n",
    "allScores = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books=dict()\n",
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>5, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(allScores)):\n",
    "    scoreTuple=(0,0,0,0)\n",
    "    s1v=avg_feature_vector(text[i],model,300,index2word_set)\n",
    "    for fl in os.listdir(potential):\n",
    "        scores=allScores[i][fl]\n",
    "        for j in range(len(scores)):\n",
    "            s2v=avg_feature_vector(books[fl][j],model,300,index2word_set)\n",
    "            semanticScore=1 - spatial.distance.cosine(s1v, s2v)\n",
    "            scoreTuples.append((i,fl,j,scores[j],semanticScore,(scores[j]+semanticScore)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples.sort(key=lambda tup: tup[5],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in scoreTuples[0:10]:\n",
    "    print('Original Sentence: ',text[t[0]])\n",
    "    print('Similar Sentence is from: ',t[1])\n",
    "    print('Score: ',t[3])\n",
    "    print(books[t[1]][t[2]])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New approach: Semantic filtering using TFIDF before parsing and final semantic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential=\"./potential/\"\n",
    "booksList=os.listdir(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"./new/matthew\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books=dict()\n",
    "for file in booksList:\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>5, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "corpus=corpus+text\n",
    "for fl in os.listdir(potential):\n",
    "    corpus=corpus+books[fl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIDFScores=[]\n",
    "for i in range(len(text)):\n",
    "    scoresDict={}\n",
    "    j=len(text)\n",
    "    for fl in booksList:\n",
    "        bookScore=[]\n",
    "        for k in range(len(books[fl])):\n",
    "#             print(k)\n",
    "            j=len(text)+k\n",
    "#             print(j)\n",
    "            simScore=1-spatial.distance.cosine(X[i].toarray(), X[j].toarray())\n",
    "            bookScore.append((simScore,k))\n",
    "        scoresDict[fl]=bookScore\n",
    "    tfIDFScores.append(scoresDict)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tfIDFScores:\n",
    "    for book in booksList:\n",
    "        sent[book]=list(filter(lambda tup: tup[0]>0.2,sent[book]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedSentences=dict()\n",
    "for book in booksList:\n",
    "    reducedSentences[book]=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tfIDFScores:\n",
    "    for book in booksList:\n",
    "        reducedSentences[book]=reducedSentences[book]+[x[1] for x in sent[book]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in booksList:\n",
    "    reducedSentences[book]=list(set(reducedSentences[book]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reducedSentences['isaiah.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedBooks=dict()\n",
    "for book in booksList:\n",
    "    reducedBooks[book]=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in booksList:\n",
    "    for sent in reducedSentences[book]:\n",
    "        reducedBooks[book].append(books[book][sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"./new/matthew\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "parseTrees=list()\n",
    "for sent in text:\n",
    "    print(i)\n",
    "    sentParse=getNLPToks(sent)\n",
    "    tempTree=tree()\n",
    "    generateTree(sentParse['parse'],tempTree)\n",
    "    flipTree(tempTree)\n",
    "    parseTrees.append(tempTree)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"./tempOutput/parseTrees.pickle\",\"rb\")\n",
    "parseTrees = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentialParseTrees=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(books['isaiah.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reducedBooks['isaiah.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in booksList:\n",
    "    print(book)\n",
    "    candidate=reducedBooks[book]\n",
    "    pTrees=list()\n",
    "    for sent in candidate:\n",
    "        sentParse=getNLPToks(sent)\n",
    "        tempTree=tree()\n",
    "        generateTree(sentParse['parse'],tempTree)\n",
    "        flipTree(tempTree)\n",
    "        pTrees.append(tempTree)\n",
    "    potentialParseTrees[book]=pTrees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScores=list()\n",
    "i=0\n",
    "for tr in parseTrees:\n",
    "#     print(i)\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    sentScoreDict=dict()\n",
    "    for book in booksList:\n",
    "#         print(file)\n",
    "        bookTrees=potentialParseTrees[book]\n",
    "        df=list()\n",
    "        for bTree in bookTrees:\n",
    "            (rscore_st, nscore_st) = MoschittiPT(tr, bTree, 0.8, 1, 1)\n",
    "            df.append(nscore_st)\n",
    "#         print(df)\n",
    "        sentScoreDict[book]=df\n",
    "    allScores.append(sentScoreDict)\n",
    "#     print('over')\n",
    "    i=i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) \n",
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(allScores)):\n",
    "    s1v=avg_feature_vector(text[i],model,300,index2word_set)\n",
    "    for fl in booksList:\n",
    "        scores=allScores[i][fl]\n",
    "        for j in range(len(scores)):\n",
    "            s2v=avg_feature_vector(reducedBooks[fl][j],model,300,index2word_set)\n",
    "            semanticScore=1 - spatial.distance.cosine(s1v, s2v)\n",
    "            scoreTuples.append((i,fl,j,scores[j],semanticScore,(scores[j]+semanticScore)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples.sort(key=lambda tup: tup[5],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in scoreTuples[0:10]:\n",
    "    print('Original Sentence: ',text[t[0]])\n",
    "    print('Similar Sentence is from: ',t[1])\n",
    "    print('Syntactic Score: ',t[3])\n",
    "    print('Semantic Score: ',t[4])\n",
    "    print(reducedBooks[t[1]][t[2]])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"./tempOutput/parseTrees.pickle\",\"rb\")\n",
    "parseTrees = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1='23 Behold, a virgin shall be with child, and shall bring forth a son, and they shall call his name Emmanuel, which being interpreted is, God with us.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=getNLPToks(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=out1['parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=''\n",
    "for i in x:\n",
    "    s=s+i\n",
    "y=Tree.fromstring(s)\n",
    "TreeView(y)._cframe.print_to_file('output.ps')\n",
    "os.system('convert output.ps output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Tree.fromstring(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TreeView(y)._cframe.print_to_file('output.ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('convert output.ps output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using jacardian index for initial filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "stopwords.append('thou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacardScore(a, b):\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    ratio = len(set(tokens_a).intersection(tokens_b)) / float(len(set(tokens_a).union(tokens_b)))\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential=\"./potential/\"\n",
    "booksList=os.listdir(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"./new/matthew\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isaiah.txt\n",
      "micah.txt\n"
     ]
    }
   ],
   "source": [
    "books=dict()\n",
    "for file in booksList:\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>5, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacardScores=[]\n",
    "for i in range(len(text)):\n",
    "    scoresDict={}\n",
    "    for book in booksList:\n",
    "        bookScore=[]\n",
    "        for k in range(len(books[book])):\n",
    "            simScore=jacardScore(text[i], books[book][k])\n",
    "            bookScore.append((simScore,k))\n",
    "        scoresDict[book]=bookScore\n",
    "    jacardScores.append(scoresDict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in jacardScores:\n",
    "    for book in booksList:\n",
    "        sent[book]=list(filter(lambda tup: tup[0]>0.15,sent[book]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedSentences=dict()\n",
    "for book in booksList:\n",
    "    reducedSentences[book]=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in jacardScores:\n",
    "    for book in booksList:\n",
    "        reducedSentences[book]=reducedSentences[book]+[x[1] for x in sent[book]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in booksList:\n",
    "    reducedSentences[book]=list(set(reducedSentences[book]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reducedSentences['isaiah.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedBooks=dict()\n",
    "for book in booksList:\n",
    "    reducedBooks[book]=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in booksList:\n",
    "    for sent in reducedSentences[book]:\n",
    "        reducedBooks[book].append(books[book][sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "parseTrees=list()\n",
    "parsedSentences=list()\n",
    "for sent in text:\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    sentParse=getNLPToks(sent)\n",
    "    tempTree=tree()\n",
    "    generateTree(sentParse['parse'],tempTree)\n",
    "    parsedSentences.append(sentParse['parse'])\n",
    "    flipTree(tempTree)\n",
    "    parseTrees.append(tempTree)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"./tempOutput/parseTrees.pickle\",\"rb\")\n",
    "parseTrees = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentialParseTrees=dict()\n",
    "potentialParsedSentences=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isaiah.txt\n",
      "micah.txt\n"
     ]
    }
   ],
   "source": [
    "for book in booksList:\n",
    "    print(book)\n",
    "    candidate=reducedBooks[book]\n",
    "    pTrees=list()\n",
    "    pSents=list()\n",
    "    for sent in candidate:\n",
    "        sentParse=getNLPToks(sent)\n",
    "        tempTree=tree()\n",
    "        generateTree(sentParse['parse'],tempTree)\n",
    "        pSents.append(sentParse['parse'])\n",
    "        flipTree(tempTree)\n",
    "        pTrees.append(tempTree)\n",
    "    potentialParseTrees[book]=pTrees\n",
    "    potentialParsedSentences[book]=pSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "allScores=list()\n",
    "i=0\n",
    "for tr in parseTrees:\n",
    "#     print(i)\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    sentScoreDict=dict()\n",
    "    for book in booksList:\n",
    "#         print(file)\n",
    "        bookTrees=potentialParseTrees[book]\n",
    "        df=list()\n",
    "        for bTree in bookTrees:\n",
    "            (rscore_st, nscore_st) = MoschittiPT(tr, bTree, 0.8, 1, 1)\n",
    "            df.append(nscore_st)\n",
    "#         print(df)\n",
    "        sentScoreDict[book]=df\n",
    "    allScores.append(sentScoreDict)\n",
    "#     print('over')\n",
    "    i=i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) \n",
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(allScores)):\n",
    "    s1v=avg_feature_vector(text[i],model,300,index2word_set)\n",
    "    for fl in booksList:\n",
    "        scores=allScores[i][fl]\n",
    "        for j in range(len(scores)):\n",
    "            s2v=avg_feature_vector(reducedBooks[fl][j],model,300,index2word_set)\n",
    "            semanticScore=1 - spatial.distance.cosine(s1v, s2v)\n",
    "            scoreTuples.append((i,fl,j,scores[j],semanticScore,(scores[j]+semanticScore)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples.sort(key=lambda tup: tup[5],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  23 Behold, a virgin shall be with child, and shall bring forth a son, and they shall call his name Emmanuel, which being interpreted is, God with us.\n",
      "Similar Sentence is from:  isaiah.txt\n",
      "Syntactic Score:  0.911016855099\n",
      "Semantic Score:  0.88051789999\n",
      "14 Therefore\n",
      "the Lord himself shall give you a sign; Behold, a virgin shall\n",
      "conceive, and bear a son, and shall call his name Immanuel.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  for it is written, Thou shalt worship the Lord thy God, and him only shalt thou serve.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Syntactic Score:  0.883703966328\n",
      "Semantic Score:  0.885921001434\n",
      "14 Thou shalt eat, but not be satisfied; and thy casting down shall\n",
      "be in the midst of thee; and thou shalt take hold, but shalt not\n",
      "deliver; and that which thou deliverest will I give up to the sword.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  21 Ye have heard that it was said by them of old time, Thou shalt not kill; and whosoever shall kill shall be in danger of the judgment.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Syntactic Score:  0.901857414322\n",
      "Semantic Score:  0.851766943932\n",
      "14 Thou shalt eat, but not be satisfied; and thy casting down shall\n",
      "be in the midst of thee; and thou shalt take hold, but shalt not\n",
      "deliver; and that which thou deliverest will I give up to the sword.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  33 Again, ye have heard that it hath been said by them of old time, Thou shalt not forswear thyself, but shalt perform unto the Lord thine oaths.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Syntactic Score:  0.88616822198\n",
      "Semantic Score:  0.866317808628\n",
      "14 Thou shalt eat, but not be satisfied; and thy casting down shall\n",
      "be in the midst of thee; and thou shalt take hold, but shalt not\n",
      "deliver; and that which thou deliverest will I give up to the sword.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  19 Whosoever therefore shall break one of these least commandments, and shall teach men so, he shall be called the least in the kingdom of heaven.\n",
      "Similar Sentence is from:  isaiah.txt\n",
      "Syntactic Score:  0.871151269445\n",
      "Semantic Score:  0.872855424881\n",
      "10 And in that day there shall be a root of Jesse, which shall\n",
      "stand for an ensign of the people; to it shall the Gentiles seek.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in scoreTuples[0:5]:\n",
    "    print('Original Sentence: ',text[t[0]])\n",
    "    print('Similar Sentence is from: ',t[1])\n",
    "    print('Syntactic Score: ',t[3])\n",
    "    print('Semantic Score: ',t[4])\n",
    "    print(reducedBooks[t[1]][t[2]])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in scoreTuples[0:5]:\n",
    "    sent1_id=t[0]\n",
    "    x=parsedSentences[sent1_id]\n",
    "    s=''\n",
    "    for i in x:\n",
    "        s=s+i\n",
    "    y=Tree.fromstring(s)\n",
    "    TreeView(y)._cframe.print_to_file('./trees/new'+str(sent1_id)+'.ps')\n",
    "    potentialBook=t[1]\n",
    "    sent2_id=t[2]\n",
    "    x=potentialParsedSentences[potentialBook][sent2_id]\n",
    "    s=''\n",
    "    for i in x:\n",
    "        s=s+i\n",
    "    y=Tree.fromstring(s)\n",
    "    TreeView(y)._cframe.print_to_file('./trees/potential'+str(sent1_id)+'.ps')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python personal",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
