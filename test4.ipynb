{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initial config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import bisect\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import os\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "from nltk.tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "public='/home/users2/mehrotsh/scripts/packages/stanford-corenlp-full-2018-02-27/'\n",
    "personal='/home/samarth/stanford-corenlp-full-2018-02-27/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(personal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree(): \n",
    "    return defaultdict(tree)\n",
    "\n",
    "\n",
    "def _leadingSpaces_(target):\n",
    "    return len(target) - len(target.lstrip())\n",
    "\n",
    "def _findParent_(curIndent, parid, treeRef):\n",
    "    tmpid = parid\n",
    "    while (curIndent <= treeRef[tmpid]['indent']):\n",
    "        tmpid = treeRef[tmpid]['parid']\n",
    "    return tmpid\n",
    "\n",
    "\n",
    "def generateTree(rawTokens, treeRef):\n",
    "\n",
    "    # (token\n",
    "    REGEX_OPEN = r\"^\\s*\\(([a-zA-Z0-9_']*)\\s*$\"\n",
    "    # (token (tok1 tok2) (tok3 tok4) .... (tokx toky))\n",
    "    REGEX_COMP = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*((?:[(]([a-zA-Z0-9_;.,?'!]+)\\s*([a-zA-Z0-9_;\\.,?!']+)[)]\\s*)+)\"    \n",
    "    # (, ,) as stand-alone. Used for match() not search()\n",
    "    REGEX_PUNC = r\"^\\s*\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "    # (tok1 tok2) as stand-alone\n",
    "    REGEX_SOLO_PAIR = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*([a-zA-Z0-9_']+)\\)\"\n",
    "    # (tok1 tok2) used in search()\n",
    "    REGEX_ISOL_IN_COMP = r\"\\(([a-zA-Z0-9_;.,?!']+)\\s*([a-zA-Z0-9_;.,?!']+)\\)\"\n",
    "    # (punc punc) used in search()\n",
    "    REGEX_PUNC_SOLO = r\"\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "   \n",
    "    treeRef[len(treeRef)] = {'curid':0, \n",
    "                             'parid':-1, \n",
    "                             'posOrTok':'ROOT', \n",
    "                             'indent':0,\n",
    "                            'children':[],\n",
    "                            'childrenTok':[]}\n",
    "    ID_CTR = 1\n",
    "    \n",
    "    for tok in rawTokens[1:]:\n",
    "        \n",
    "        curIndent = _leadingSpaces_(tok) \n",
    "        parid = _findParent_(curIndent, ID_CTR-1, treeRef)\n",
    "        \n",
    "        # CHECK FOR COMPOSITE TOKENS\n",
    "        checkChild = re.match(REGEX_COMP, tok)\n",
    "        if (checkChild):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkChild.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            upCTR = ID_CTR\n",
    "            ID_CTR += 1\n",
    "            \n",
    "            subCheck = re.sub(REGEX_PUNC_SOLO,'',checkChild.group(2))\n",
    "            subs = re.findall(REGEX_ISOL_IN_COMP, subCheck) \n",
    "            for ch in subs:\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':upCTR, \n",
    "                                   'posOrTok':ch[0], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':ID_CTR-1, \n",
    "                                   'posOrTok':ch[1], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "            continue\n",
    "           \n",
    "\n",
    "            \n",
    "        checkSingle = re.match(REGEX_SOLO_PAIR, tok)\n",
    "        if (checkSingle):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkSingle.group(1), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':ID_CTR-1, \n",
    "                               'posOrTok':checkSingle.group(2), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        checkPunc = re.match(REGEX_PUNC, tok)\n",
    "        if (checkPunc): # ignore punctuation\n",
    "            continue\n",
    "\n",
    "        checkMatch = re.match(REGEX_OPEN, tok)\n",
    "        if (checkMatch):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkMatch.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "\n",
    "    return\n",
    "            \n",
    "\n",
    "def flipTree(treeRef):\n",
    "    # Pass 1 fill in children\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            bisect.insort(treeRef[v['parid']]['children'], k)\n",
    "    # Pass 2 map children to tokens\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            treeRef[k]['childrenTok'] = [treeRef[ch]['posOrTok'] for ch in treeRef[k]['children']]\n",
    "    treeRef[0]['childrenTok'] = treeRef[1]['posOrTok']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _isLeaf_(tree, parentNode):\n",
    "    return (len(tree[parentNode]['children']) == 0)\n",
    "\n",
    "def _isPreterminal_(tree, parentNode):\n",
    "    for idx in tree[parentNode]['children']:\n",
    "        if not _isLeaf_(tree, idx):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel\n",
    "'''\n",
    "\n",
    "def _cdHelper_(tree1, tree2, node1, node2, store, lam, SST_ON):\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "        # same children tokens\n",
    "        if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "            # Check if both nodes are pre-terminal\n",
    "            if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "                store[node1, node2] = lam\n",
    "                return\n",
    "            # Not pre-terminal. Recurse among the children of both token trees.\n",
    "            else:\n",
    "                nChildren = len(tree1[node1]['children'])\n",
    "\n",
    "                runningTotal = None\n",
    "                for idx in range(nChildren):\n",
    "                     # index ->  node_id\n",
    "                    tmp_n1 = tree1[node1]['children'][idx]\n",
    "                    tmp_n2 = tree2[node2]['children'][idx]\n",
    "                    # Recursively run helper\n",
    "                    _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON)\n",
    "                    # Set the initial value for the layer. Else multiplicative product.\n",
    "                    if (runningTotal == None):\n",
    "                        runningTotal = SST_ON + store[tmp_n1, tmp_n2]\n",
    "                    else:\n",
    "                        runningTotal *= (SST_ON + store[tmp_n1, tmp_n2])\n",
    "\n",
    "                store[node1, node2] = lam * runningTotal\n",
    "                return\n",
    "        else:\n",
    "            store[node1, node2] = 0\n",
    "    else: # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "\n",
    "def _cdKernel_(tree1, tree2, lam, SST_ON):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def CollinsDuffy(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON):\n",
    "    raw_score = _cdKernel_(tree1, tree2, lam, SST_ON)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _cdKernel_(tree1, tree1, lam, SST_ON)\n",
    "        t2_score = _cdKernel_(tree2, tree2, lam, SST_ON)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Implementation of the Partial Tree (PT) Kernel from:\n",
    "\"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees\"\n",
    "by Alessandro Moschitti\n",
    "'''\n",
    "\n",
    "'''\n",
    "The delta function is stolen from the Collins-Duffy kernel\n",
    "'''\n",
    "\n",
    "def _deltaP_(tree1, tree2, seq1, seq2, store, lam, mu, p):\n",
    "\n",
    "#     # Enumerate subsequences of length p+1 for each child set\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # generate delta(a,b)\n",
    "        _delta_(tree1, tree2, seq1[-1], seq2[-1], store, lam, mu)\n",
    "        if store[seq1[-1], seq2[-1]] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            runningTot = 0\n",
    "            for i in range(p-1, len(seq1)-1):\n",
    "                for r in range(p-1, len(seq2)-1):\n",
    "                    scaleFactor = pow(lam, len(seq1[:-1])-i+len(seq2[:-1])-r)\n",
    "                    dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-1)\n",
    "                    runningTot += (scaleFactor * dp)\n",
    "            return runningTot\n",
    "\n",
    "def _delta_(tree1, tree2, node1, node2, store, lam, mu):\n",
    "\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "\n",
    "        if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "            if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "                store[node1, node2] = lam\n",
    "            else:\n",
    "                store[node1, node2] = 0\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # establishes p_max\n",
    "            childmin = min(len(tree1[node1]['children']), len(tree2[node2]['children']))\n",
    "            deltaTot = 0\n",
    "            for p in range(1,childmin+1):\n",
    "                # compute delta_p\n",
    "                deltaTot += _deltaP_(tree1, tree2,\n",
    "                                     tree1[node1]['children'],\n",
    "                                     tree2[node2]['children'], store, lam, mu, p)\n",
    "\n",
    "            store[node1, node2] = mu * (pow(lam,2) + deltaTot)\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "def _ptKernel_(tree1, tree2, lam, mu):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _delta_(tree1, tree2, i, j, store, lam, mu)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def MoschittiPT(tree1, tree2, lam, mu, NORMALIZE_FLAG):\n",
    "    raw_score = _ptKernel_(tree1, tree2, lam, mu)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _ptKernel_(tree1, tree1, lam, mu)\n",
    "        t2_score = _ptKernel_(tree2, tree2, lam, mu)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNLPToks(rawSentence):\n",
    "    output = nlp.annotate(rawSentence, properties={'annotators': 'tokenize,ssplit,pos,parse','outputFormat': 'json','timeout':'50000'})\n",
    "    output=ast.literal_eval(output)\n",
    "    tokens = output['sentences'][0]['tokens']\n",
    "    parse = output['sentences'][0]['parse'].split(\"\\n\")\n",
    "    return {\n",
    "        'toks':tokens, 'parse':parse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDuffyScore(sent1,sent2):\n",
    "    tree_1=tree()\n",
    "    tree_2=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    out2=getNLPToks(sent2)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    generateTree(out2['parse'],tree_2)\n",
    "    flipTree(tree_1)\n",
    "    flipTree(tree_2)\n",
    "    (rscore_st, nscore_st) = CollinsDuffy(tree_1, tree_2, 0.8, 1, 1)\n",
    "    return rscore_st,nscore_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMoschittiScore(sent1,sent2):\n",
    "    tree_1=tree()\n",
    "    tree_2=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    out2=getNLPToks(sent2)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    generateTree(out2['parse'],tree_2)\n",
    "    flipTree(tree_1)\n",
    "    flipTree(tree_2)\n",
    "    (rscore_st, nscore_st) = MoschittiPT(tree_1, tree_2, 0.8, 1, 1)\n",
    "#     return rscore_st,nscore_st\n",
    "    return nscore_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing on Project Gutenberg samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating parse trees for the new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=\"./new/pierre.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>1, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "parseTrees=list()\n",
    "for sent in text:\n",
    "    print(i)\n",
    "    sentParse=getNLPToks(sent)\n",
    "    tempTree=tree()\n",
    "    generateTree(sentParse['parse'],tempTree)\n",
    "    flipTree(tempTree)\n",
    "    parseTrees.append(tempTree)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(parseTrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading candidates and creating parse trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potential=\"./potential/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potentialParseTrees=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>1, candidate))\n",
    "    pTrees=list()\n",
    "    for sent in candidate:\n",
    "        sentParse=getNLPToks(sent)\n",
    "        tempTree=tree()\n",
    "        generateTree(sentParse['parse'],tempTree)\n",
    "        flipTree(tempTree)\n",
    "        pTrees.append(tempTree)\n",
    "    potentialParseTrees[file]=pTrees\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allScores=list()\n",
    "i=0\n",
    "for tr in parseTrees:\n",
    "#     print(i)\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    sentScoreDict=dict()\n",
    "    for file in os.listdir(potential):\n",
    "#         print(file)\n",
    "        bookTrees=potentialParseTrees[file]\n",
    "        df=list()\n",
    "        for bTree in bookTrees:\n",
    "            (rscore_st, nscore_st) = MoschittiPT(tr, bTree, 0.8, 1, 1)\n",
    "            df.append(nscore_st)\n",
    "#         print(df)\n",
    "        sentScoreDict[file]=df\n",
    "    allScores.append(sentScoreDict)\n",
    "#     print('over')\n",
    "    i=i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(allScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text[174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allScores=allScores[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books=dict()\n",
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>1, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(60,70):\n",
    "    print('Sentence',i)\n",
    "    print('Original Sent',text[i])\n",
    "    for book in os.listdir(potential):\n",
    "        print(book)\n",
    "        maxIndex=allScores[i][book].index(max(allScores[i][book]))\n",
    "        print('Score',allScores[i][book][maxIndex])\n",
    "        print('Similar sentence:',books[book][maxIndex])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allScores[600]['2.txt'].index(max(allScores[600]['2.txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(allScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(allScores[0]['5.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(allScores)):\n",
    "    scoreTuple=(0,0,0,0)\n",
    "    for fl in os.listdir(potential):\n",
    "        scores=allScores[i][fl]\n",
    "        for j in range(len(scores)):\n",
    "            scoreTuples.append((i,fl,j,scores[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreTuples.sort(key=lambda tup: tup[3],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(scoreTuples[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing on Bible sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two related sentences - high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent1='Behold, a virgin shall conceive and bear a son, and his name shall be called Emmanuel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent2='behold, a virgin shall conceive in the womb, and shall bring forth a son, and thou shalt call his name Emmanuel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90231868985626051"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent1,sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two related sentences - high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent3='And thou, Bethlehem, in the land of Juda, art not the least among the princes of Juda: for out of thee shall come a Governor, that shall rule my people Israel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent4='And thou, Bethleem, house of Ephratha, art few in number to be reckoned among the thousands of Juda; yet out of thee shall one come forth to me, to be a ruler of Israel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93671489621861459"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent3,sent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sentences that are not highly related, not such a high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65386364458884894"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent1,sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent5='In Rama was there a voice heard, lamentation, and weeping, and great mourning, Rachel weeping for her children, and would not be comforted because they are not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent6='A voice was heard in Rama, of lamentation, and of weeping, and wailing; Rachel would not cease weeping for her children, because they are not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87978779877927293"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent5,sent6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68938859080577475"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent5,sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent7=' Then saith Jesus unto them, All ye shall be offended because of me this night: for it is written, I will smite the shepherd, and the sheep of the flock shall be scattered abroad.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent8='Awake, O sword, against my shepherds, and against the man who is my citizen, saith the Lord Almighty: smite the shepherds, and draw out the sheep: and I will bring mine hand upon the little ones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86443556016593959"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent7,sent8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very similar but still a reasonably high score (False positive), might be a parsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79324678818767691"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent7,sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent9='Jesus said unto him, Thou shalt love the Lord thy God with all thy heart, and with all thy soul, and with all thy mind.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent10='And thou shalt love the Lord thy God with all thy mind, and with all thy soul, and all thy strength'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93498567308925917"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent9,sent10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65811851800780774"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMoschittiScore(sent9,sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on chunks of the bible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potential=\"./potential/\"\n",
    "booksList=os.listdir(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=\"./new/matthew\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "parseTrees=list()\n",
    "for sent in text:\n",
    "    print(i)\n",
    "    sentParse=getNLPToks(sent)\n",
    "    tempTree=tree()\n",
    "    generateTree(sentParse['parse'],tempTree)\n",
    "    flipTree(tempTree)\n",
    "    parseTrees.append(tempTree)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling_on = open(\"./tempOutput/parseTrees.pickle\",\"wb\")\n",
    "pickle.dump(parseTrees, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potential=\"./potential/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potentialParseTrees=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isaiah.txt\n",
      "micah.txt\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>5, candidate))\n",
    "    pTrees=list()\n",
    "    for sent in candidate:\n",
    "        sentParse=getNLPToks(sent)\n",
    "        tempTree=tree()\n",
    "        generateTree(sentParse['parse'],tempTree)\n",
    "        flipTree(tempTree)\n",
    "        pTrees.append(tempTree)\n",
    "    potentialParseTrees[file]=pTrees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling_on = open(\"./tempOutput/potentialParseTrees.pickle\",\"wb\")\n",
    "pickle.dump(potentialParseTrees, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "allScores=list()\n",
    "i=0\n",
    "for tr in parseTrees:\n",
    "#     print(i)\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    sentScoreDict=dict()\n",
    "    for file in os.listdir(potential):\n",
    "#         print(file)\n",
    "        bookTrees=potentialParseTrees[file]\n",
    "        df=list()\n",
    "        for bTree in bookTrees:\n",
    "            (rscore_st, nscore_st) = MoschittiPT(tr, bTree, 0.8, 1, 1)\n",
    "            df.append(nscore_st)\n",
    "#         print(df)\n",
    "        sentScoreDict[file]=df\n",
    "    allScores.append(sentScoreDict)\n",
    "#     print('over')\n",
    "    i=i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling_on = open(\"./tempOutput/allScores.pickle\",\"wb\")\n",
    "pickle.dump(allScores, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_off = open(\"./tempOutput/allScores.pickle\",\"rb\")\n",
    "allScores = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isaiah.txt\n",
      "micah.txt\n"
     ]
    }
   ],
   "source": [
    "books=dict()\n",
    "for file in os.listdir(potential):\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>5, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(allScores)):\n",
    "    scoreTuple=(0,0,0,0)\n",
    "    s1v=avg_feature_vector(text[i],model,300,index2word_set)\n",
    "    for fl in os.listdir(potential):\n",
    "        scores=allScores[i][fl]\n",
    "        for j in range(len(scores)):\n",
    "            s2v=avg_feature_vector(books[fl][j],model,300,index2word_set)\n",
    "            semanticScore=1 - spatial.distance.cosine(s1v, s2v)\n",
    "            scoreTuples.append((i,fl,j,scores[j],semanticScore,(scores[j]+semanticScore)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140504"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreTuples.sort(key=lambda tup: tup[5],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(99,\n",
       "  'micah.txt',\n",
       "  91,\n",
       "  0.8762579290517688,\n",
       "  0.90674972534179688,\n",
       "  0.89150382719678278),\n",
       " (96,\n",
       "  'isaiah.txt',\n",
       "  228,\n",
       "  0.92158759035369442,\n",
       "  0.8383827805519104,\n",
       "  0.87998518545280247),\n",
       " (21,\n",
       "  'isaiah.txt',\n",
       "  270,\n",
       "  0.90639150246639377,\n",
       "  0.84649938344955444,\n",
       "  0.87644544295797411),\n",
       " (21,\n",
       "  'micah.txt',\n",
       "  52,\n",
       "  0.89984788878418975,\n",
       "  0.84785234928131104,\n",
       "  0.87385011903275034),\n",
       " (21,\n",
       "  'micah.txt',\n",
       "  129,\n",
       "  0.89951541752032949,\n",
       "  0.84728765487670898,\n",
       "  0.87340153619851923),\n",
       " (156,\n",
       "  'micah.txt',\n",
       "  74,\n",
       "  0.90657272704207958,\n",
       "  0.838206946849823,\n",
       "  0.87238983694595129),\n",
       " (21,\n",
       "  'micah.txt',\n",
       "  182,\n",
       "  0.90725018359973664,\n",
       "  0.83577513694763184,\n",
       "  0.87151266027368424),\n",
       " (51,\n",
       "  'isaiah.txt',\n",
       "  361,\n",
       "  0.88115923772555071,\n",
       "  0.86115968227386475,\n",
       "  0.87115945999970767),\n",
       " (21,\n",
       "  'isaiah.txt',\n",
       "  173,\n",
       "  0.90083547093605454,\n",
       "  0.83656942844390869,\n",
       "  0.86870244968998156),\n",
       " (46,\n",
       "  'isaiah.txt',\n",
       "  52,\n",
       "  0.8981454293009028,\n",
       "  0.8379817008972168,\n",
       "  0.8680635650990598)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreTuples[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  for it is written, Thou shalt worship the Lord thy God, and him only shalt thou serve.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Score:  0.876257929052\n",
      "for now shalt thou go forth out of the city, and\n",
      "thou shalt dwell in the field, and thou shalt go even to Babylon;\n",
      "there shalt thou be delivered; there the LORD shall redeem thee from\n",
      "the hand of thine enemies.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  8 Again, the devil taketh him up into an exceeding high mountain, and sheweth him all the kingdoms of the world, and the glory of them; 4.\n",
      "Similar Sentence is from:  isaiah.txt\n",
      "Score:  0.921587590354\n",
      "7 Now therefore, behold, the Lord bringeth\n",
      "up upon them the waters of the river, strong and many, even the king\n",
      "of Assyria, and all his glory.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  20 But while he thought on these things, behold, the angel of the LORD appeared unto him in a dream, saying, Joseph, thou son of David, fear not to take unto thee Mary thy wife.\n",
      "Similar Sentence is from:  isaiah.txt\n",
      "Score:  0.906391502466\n",
      "13 For the people turneth not unto him that smiteth them, neither do\n",
      "they seek the LORD of hosts.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  20 But while he thought on these things, behold, the angel of the LORD appeared unto him in a dream, saying, Joseph, thou son of David, fear not to take unto thee Mary thy wife.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Score:  0.899847888784\n",
      "11 If a man walking in the spirit and falsehood do lie, saying, I\n",
      "will prophesy unto thee of wine and of strong drink; he shall even be\n",
      "the prophet of this people.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  20 But while he thought on these things, behold, the angel of the LORD appeared unto him in a dream, saying, Joseph, thou son of David, fear not to take unto thee Mary thy wife.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Score:  0.89951541752\n",
      "5 O my people, remember now what Balak king of Moab consulted, and\n",
      "what Balaam the son of Beor answered him from Shittim unto Gilgal;\n",
      "that ye may know the righteousness of the LORD.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  and whosoever shall say to his brother, Raca, shall be in danger of the council.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Score:  0.906572727042\n",
      "1 But in the last days it shall come to pass, that the mountain of\n",
      "the house of the LORD shall be established in the top of the\n",
      "mountains, and it shall be exalted above the hills; and people shall\n",
      "flow unto it.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  20 But while he thought on these things, behold, the angel of the LORD appeared unto him in a dream, saying, Joseph, thou son of David, fear not to take unto thee Mary thy wife.\n",
      "Similar Sentence is from:  micah.txt\n",
      "Score:  0.9072501836\n",
      "20 Thou wilt perform the truth to Jacob, and the mercy to Abraham,\n",
      "which thou hast sworn unto our fathers from the days of old.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  16 Then Herod, when he saw that he was mocked of the wise men, was exceeding wroth, and sent forth, and slew all the children that were in Bethlehem, and in all the coasts thereof, from two years old and under, according to the time which he had diligently enquired of the wise men.\n",
      "Similar Sentence is from:  isaiah.txt\n",
      "Score:  0.881159237726\n",
      "16 And there shall be an highway for the remnant of his people,\n",
      "which shall be left, from Assyria; like as it was to Israel in the day\n",
      "that he came up out of the land of Egypt.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  20 But while he thought on these things, behold, the angel of the LORD appeared unto him in a dream, saying, Joseph, thou son of David, fear not to take unto thee Mary thy wife.\n",
      "Similar Sentence is from:  isaiah.txt\n",
      "Score:  0.900835470936\n",
      "3 And one cried unto another, and said, Holy, holy, holy, is the\n",
      "LORD of hosts.\n",
      "\n",
      "\n",
      "\n",
      "Original Sentence:  13 And when they were departed, behold, the angel of the Lord appeareth to Joseph in a dream, saying, Arise, and take the young child and his mother, and flee into Egypt, and be thou there until I bring thee word.\n",
      "Similar Sentence is from:  isaiah.txt\n",
      "Score:  0.898145429301\n",
      "3 And many people shall go and say, Come ye, and let us go up to the\n",
      "mountain of the LORD, to the house of the God of Jacob; and he will\n",
      "teach us of his ways, and we will walk in his paths.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in scoreTuples[0:10]:\n",
    "    print('Original Sentence: ',text[t[0]])\n",
    "    print('Similar Sentence is from: ',t[1])\n",
    "    print('Score: ',t[3])\n",
    "    print(books[t[1]][t[2]])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New approach: Semantic filtering using TFIDF before parsing and final semantic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=\"./new/matthew\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isaiah.txt\n",
      "micah.txt\n"
     ]
    }
   ],
   "source": [
    "books=dict()\n",
    "for file in booksList:\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>5, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "corpus=corpus+text\n",
    "for fl in os.listdir(potential):\n",
    "    corpus=corpus+books[fl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 2171)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfIDFScores=[]\n",
    "for i in range(len(text)):\n",
    "    scoresDict={}\n",
    "    j=len(text)\n",
    "    for fl in booksList:\n",
    "        bookScore=[]\n",
    "        for k in range(len(books[fl])):\n",
    "#             print(k)\n",
    "            j=len(text)+k\n",
    "#             print(j)\n",
    "            simScore=1-spatial.distance.cosine(X[i].toarray(), X[j].toarray())\n",
    "            bookScore.append((simScore,k))\n",
    "        scoresDict[fl]=bookScore\n",
    "    tfIDFScores.append(scoresDict)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in tfIDFScores:\n",
    "    for book in booksList:\n",
    "        sent[book]=list(filter(lambda tup: tup[0]>0.3,sent[book]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfIDFScores[0]['isaiah.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reducedSentences=dict()\n",
    "for book in booksList:\n",
    "    reducedSentences[book]=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in tfIDFScores:\n",
    "    for book in booksList:\n",
    "        reducedSentences[book]=reducedSentences[book]+[x[1] for x in sent[book]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for book in booksList:\n",
    "    reducedSentences[book]=list(set(reducedSentences[book]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reducedSentences['micah.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reducedBooks=dict()\n",
    "for book in booksList:\n",
    "    reducedBooks[book]=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for book in booksList:\n",
    "    for sent in reducedSentences[book]:\n",
    "        reducedBooks[book].append(books[book][sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=\"./new/matthew\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "parseTrees=list()\n",
    "for sent in text:\n",
    "    print(i)\n",
    "    sentParse=getNLPToks(sent)\n",
    "    tempTree=tree()\n",
    "    generateTree(sentParse['parse'],tempTree)\n",
    "    flipTree(tempTree)\n",
    "    parseTrees.append(tempTree)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_off = open(\"./tempOutput/parseTrees.pickle\",\"rb\")\n",
    "parseTrees = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potentialParseTrees=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isaiah.txt\n",
      "micah.txt\n"
     ]
    }
   ],
   "source": [
    "for book in booksList:\n",
    "    print(book)\n",
    "    candidate=reducedBooks[book]\n",
    "    pTrees=list()\n",
    "    for sent in candidate:\n",
    "        sentParse=getNLPToks(sent)\n",
    "        tempTree=tree()\n",
    "        generateTree(sentParse['parse'],tempTree)\n",
    "        flipTree(tempTree)\n",
    "        pTrees.append(tempTree)\n",
    "    potentialParseTrees[book]=pTrees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
