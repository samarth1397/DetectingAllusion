{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring results from various data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import bisect\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import os\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "from nltk.tree import *\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import nltk.stem.snowball\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "from nltk.draw.tree import TreeView\n",
    "from fuzzywuzzy import fuzz\n",
    "from multiprocessing import Pool\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import wordnet \n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(): \n",
    "    return defaultdict(tree)\n",
    "\n",
    "\n",
    "def _leadingSpaces_(target):\n",
    "    return len(target) - len(target.lstrip())\n",
    "\n",
    "def _findParent_(curIndent, parid, treeRef):\n",
    "    tmpid = parid\n",
    "    while (curIndent <= treeRef[tmpid]['indent']):\n",
    "        tmpid = treeRef[tmpid]['parid']\n",
    "    return tmpid\n",
    "\n",
    "\n",
    "def generateTree(rawTokens, treeRef):\n",
    "\n",
    "    # (token\n",
    "    REGEX_OPEN = r\"^\\s*\\(([a-zA-Z0-9_']*)\\s*$\"\n",
    "    # (token (tok1 tok2) (tok3 tok4) .... (tokx toky))\n",
    "    REGEX_COMP = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*((?:[(]([a-zA-Z0-9_;.,?'!]+)\\s*([a-zA-Z0-9_;\\.,?!']+)[)]\\s*)+)\"    \n",
    "    # (, ,) as stand-alone. Used for match() not search()\n",
    "    REGEX_PUNC = r\"^\\s*\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "    # (tok1 tok2) as stand-alone\n",
    "    REGEX_SOLO_PAIR = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*([a-zA-Z0-9_']+)\\)\"\n",
    "    # (tok1 tok2) used in search()\n",
    "    REGEX_ISOL_IN_COMP = r\"\\(([a-zA-Z0-9_;.,?!']+)\\s*([a-zA-Z0-9_;.,?!']+)\\)\"\n",
    "    # (punc punc) used in search()\n",
    "    REGEX_PUNC_SOLO = r\"\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "   \n",
    "    treeRef[len(treeRef)] = {'curid':0, \n",
    "                             'parid':-1, \n",
    "                             'posOrTok':'ROOT', \n",
    "                             'indent':0,\n",
    "                            'children':[],\n",
    "                            'childrenTok':[]}\n",
    "    ID_CTR = 1\n",
    "    \n",
    "    for tok in rawTokens[1:]:\n",
    "        \n",
    "        curIndent = _leadingSpaces_(tok) \n",
    "        parid = _findParent_(curIndent, ID_CTR-1, treeRef)\n",
    "        \n",
    "        # CHECK FOR COMPOSITE TOKENS\n",
    "        checkChild = re.match(REGEX_COMP, tok)\n",
    "        if (checkChild):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkChild.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            upCTR = ID_CTR\n",
    "            ID_CTR += 1\n",
    "            \n",
    "            subCheck = re.sub(REGEX_PUNC_SOLO,'',checkChild.group(2))\n",
    "            subs = re.findall(REGEX_ISOL_IN_COMP, subCheck) \n",
    "            for ch in subs:\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':upCTR, \n",
    "                                   'posOrTok':ch[0], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':ID_CTR-1, \n",
    "                                   'posOrTok':ch[1], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "            continue\n",
    "           \n",
    "\n",
    "            \n",
    "        checkSingle = re.match(REGEX_SOLO_PAIR, tok)\n",
    "        if (checkSingle):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkSingle.group(1), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':ID_CTR-1, \n",
    "                               'posOrTok':checkSingle.group(2), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        checkPunc = re.match(REGEX_PUNC, tok)\n",
    "        if (checkPunc): # ignore punctuation\n",
    "            continue\n",
    "\n",
    "        checkMatch = re.match(REGEX_OPEN, tok)\n",
    "        if (checkMatch):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkMatch.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "\n",
    "    return\n",
    "            \n",
    "\n",
    "def flipTree(treeRef):\n",
    "    # Pass 1 fill in children\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            bisect.insort(treeRef[v['parid']]['children'], k)\n",
    "    # Pass 2 map children to tokens\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            treeRef[k]['childrenTok'] = [treeRef[ch]['posOrTok'] for ch in treeRef[k]['children']]\n",
    "    treeRef[0]['childrenTok'] = treeRef[1]['posOrTok']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isLeaf_(tree, parentNode):\n",
    "    return (len(tree[parentNode]['children']) == 0)\n",
    "\n",
    "def _isPreterminal_(tree, parentNode):\n",
    "    for idx in tree[parentNode]['children']:\n",
    "        if not _isLeaf_(tree, idx):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel\n",
    "'''\n",
    "\n",
    "def _cdHelper_(tree1, tree2, node1, node2, store, lam, SST_ON):\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "        # same children tokens\n",
    "        if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "            # Check if both nodes are pre-terminal\n",
    "            if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "                store[node1, node2] = lam\n",
    "                return\n",
    "            # Not pre-terminal. Recurse among the children of both token trees.\n",
    "            else:\n",
    "                nChildren = len(tree1[node1]['children'])\n",
    "\n",
    "                runningTotal = None\n",
    "                for idx in range(nChildren):\n",
    "                     # index ->  node_id\n",
    "                    tmp_n1 = tree1[node1]['children'][idx]\n",
    "                    tmp_n2 = tree2[node2]['children'][idx]\n",
    "                    # Recursively run helper\n",
    "                    _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON)\n",
    "                    # Set the initial value for the layer. Else multiplicative product.\n",
    "                    if (runningTotal == None):\n",
    "                        runningTotal = SST_ON + store[tmp_n1, tmp_n2]\n",
    "                    else:\n",
    "                        runningTotal *= (SST_ON + store[tmp_n1, tmp_n2])\n",
    "\n",
    "                store[node1, node2] = lam * runningTotal\n",
    "                return\n",
    "        else:\n",
    "            store[node1, node2] = 0\n",
    "    else: # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "\n",
    "def _cdKernel_(tree1, tree2, lam, SST_ON):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def CollinsDuffy(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON):\n",
    "    raw_score = _cdKernel_(tree1, tree2, lam, SST_ON)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _cdKernel_(tree1, tree1, lam, SST_ON)\n",
    "        t2_score = _cdKernel_(tree2, tree2, lam, SST_ON)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Implementation of the Partial Tree (PT) Kernel from:\n",
    "\"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees\"\n",
    "by Alessandro Moschitti\n",
    "'''\n",
    "\n",
    "'''\n",
    "The delta function is stolen from the Collins-Duffy kernel\n",
    "'''\n",
    "\n",
    "def _deltaP_(tree1, tree2, seq1, seq2, store, lam, mu, p):\n",
    "\n",
    "#     # Enumerate subsequences of length p+1 for each child set\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # generate delta(a,b)\n",
    "        _delta_(tree1, tree2, seq1[-1], seq2[-1], store, lam, mu)\n",
    "        if store[seq1[-1], seq2[-1]] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            runningTot = 0\n",
    "            for i in range(p-1, len(seq1)-1):\n",
    "                for r in range(p-1, len(seq2)-1):\n",
    "                    scaleFactor = pow(lam, len(seq1[:-1])-i+len(seq2[:-1])-r)\n",
    "                    dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-1)\n",
    "                    runningTot += (scaleFactor * dp)\n",
    "            return runningTot\n",
    "\n",
    "def _delta_(tree1, tree2, node1, node2, store, lam, mu):\n",
    "\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "\n",
    "        if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "            if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "                store[node1, node2] = lam\n",
    "            else:\n",
    "                store[node1, node2] = 0\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # establishes p_max\n",
    "            childmin = min(len(tree1[node1]['children']), len(tree2[node2]['children']))\n",
    "            deltaTot = 0\n",
    "            for p in range(1,childmin+1):\n",
    "                # compute delta_p\n",
    "                deltaTot += _deltaP_(tree1, tree2,\n",
    "                                     tree1[node1]['children'],\n",
    "                                     tree2[node2]['children'], store, lam, mu, p)\n",
    "\n",
    "            store[node1, node2] = mu * (pow(lam,2) + deltaTot)\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "def _ptKernel_(tree1, tree2, lam, mu):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _delta_(tree1, tree2, i, j, store, lam, mu)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def MoschittiPT(tree1, tree2, lam, mu, NORMALIZE_FLAG):\n",
    "    raw_score = _ptKernel_(tree1, tree2, lam, mu)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _ptKernel_(tree1, tree1, lam, mu)\n",
    "        t2_score = _ptKernel_(tree2, tree2, lam, mu)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jacardNouns(sent1,sent2):\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent1)):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent2)):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "#     print(nouns1)\n",
    "#     print(nouns2)\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "def jacardVerbs(sent1,sent2):\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent1)):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent2)):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "#     print(nouns1)\n",
    "#     print(nouns2)\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "def jacardAdj(sent1,sent2):\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent1)):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent2)):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "#     print(nouns1)\n",
    "#     print(nouns2)\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeTuples(scoreTuples):\n",
    "    newTupes=list()\n",
    "    for t in scoreTuples:\n",
    "        newTup=(t[0],t[1],t[2],t[3],t[4],t[5],t[6],t[7],(t[3]+t[5])/2,t[9],t[10],t[11])\n",
    "        newTupes.append(newTup)\n",
    "    return newTupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef finalFiltering(scoreTuples,reducedBooks,threshold=0.85):\n",
    "\t\ttotalPotentialSentences=0\n",
    "\t\tfor bk in booksList:\n",
    "\t\t\ttotalPotentialSentences=totalPotentialSentences+len(reducedBooks[bk])\n",
    "\t\tscoreTuples.sort(key=lambda tup: tup[0])\n",
    "\t\tfinalTuples=list()\n",
    "\t\tk=0\n",
    "\t\ti=0\n",
    "\t\twhile i<len(scoreTuples):\n",
    "\t\t\tsenttups=scoreTuples[i:i+totalPotentialSentences]\n",
    "\t\t\tsenttups.sort(key=lambda tup: tup[8],reverse=True)\n",
    "\t\t\tif senttups[0][8]>threshold:\n",
    "\t\t\t\tfinalTuples.append(senttups[0])\n",
    "# \t\t\t\tfinalTuples.append(senttups[1])\n",
    "# \t\t\t\tfinalTuples.append(senttups[2])\n",
    "\t\t\ti=i+totalPotentialSentences\n",
    "\t\t\tk=k+1\n",
    "\n",
    "\t\tfinalTuples.sort(key=lambda tup: tup[8])\n",
    "\n",
    "\t\tdiffTuples=list()\n",
    "\t\tfor tup in scoreTuples:\n",
    "\t\t\tif (tup[3]>0.8 and abs(tup[3]-tup[4])>=0.12) or (tup[4]>0.8 and abs(tup[3]-tup[4])>=0.12):\n",
    "\t\t\t\tdiffTuples.append(tup)\n",
    "\n",
    "\t\treturn finalTuples,diffTuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef nounBasedRanking(finalTuples,text,reducedBooks):\n",
    "\t\tnewTuples=list()\n",
    "\t\tfor tup in finalTuples:\n",
    "\t\t\toriginalSent=text[tup[0]]\n",
    "\t\t\trefSent=reducedBooks[tup[1]][tup[2]]\n",
    "\t\t\tnounScore=jacardNouns(originalSent,refSent)\n",
    "\t\t\tverbScore=jacardVerbs(originalSent,refSent)\n",
    "\t\t\tadjScore=jacardAdj(originalSent,refSent)\n",
    "\t\t\tnewTuples.append(tup+(nounScore,verbScore,adjScore))\n",
    "\t\tnewTuples.sort(key=itemgetter(12,8),reverse=True)\n",
    "\t\treturn newTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef writeOutput(newTuples,text,reducedBooks,fileName):\n",
    "\t\tf=open(fileName,'w')\n",
    "\t\ti=1\n",
    "\t\tlines=list()\n",
    "\t\tfor t in newTuples:\n",
    "\t\t\tj=str(i)\n",
    "\t\t\tlines.append('Pairing: '+j)\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('New Sentence: '+text[t[0]])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Reference: \\n'+reducedBooks[t[1]][t[2]])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Similar Sentence is from: '+str(t[1]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Syntactic Score: '+str(t[3]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Syntactic Similarity without tokens: '+str(t[11]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic Score: '+str(t[4]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic Score without stopwords: '+str(t[5]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('LCS Length: '+str(t[9]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('LCS: '+t[10])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common nouns: '+str(t[12]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common verbs: '+str(t[13]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common adjectives: '+str(t[14]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic similarity nouns: '+str(t[6]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic similarity verbs: '+str(t[7]))\n",
    "\t\t\tlines.append('\\n\\n')\n",
    "\t\t\ti=i+1\n",
    "\t\tf.writelines(lines)\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe/new/poe-pit-110.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=changeTuples(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe/'+'nounSortedSentencePairs5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-2/new/poe-colloquy-675.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-2/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-2/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-2/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401650"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-2/'+'nounSortedSentencePairs5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-3/new/poe-black-670.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-3/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132488"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-3/'+'nounSortedSentencePairs4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-4/new/poe-power-699.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-4/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-4/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-4/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675630"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-4/'+'nounSortedSentencePairs2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-5/new/poe-domain-679.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-5/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-5/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-5/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1719103"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-5/'+'nounSortedSentencePairs2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-3/new/poe-black-670.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-3/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3-2/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3-2/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1342845"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-3-2/'+'nounSortedSentencePairs3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for paragraph approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=spacy.load('en',disable=['parser','ner','textcat','entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef splitNewPara(text,numOfSents=3):\n",
    "\t\ti=0\n",
    "\t\ttextPara=list()\n",
    "\t\twhile(i<len(text)):\n",
    "\t\t\tif((i+numOfSents)<len(text)):\n",
    "\t\t\t\tpara=text[i:i+numOfSents]\n",
    "\t\t\t\tpara=\" \".join(para)\n",
    "\t\t\t\ti=i+1\n",
    "\t\t\t\ttextPara.append(para)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpara=text[i:len(text)]\n",
    "\t\t\t\tpara=\" \".join(para)\n",
    "\t\t\t\ttextPara.append(para)\n",
    "\t\t\t\tbreak\n",
    "\t\treturn textPara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef finalFiltering(scoreTuples,reducedParagraphs,threshold=0.89):\n",
    "\t\ttotalPotentialSentences=0\n",
    "\t\tfor bk in booksList:\n",
    "\t\t\ttotalPotentialSentences=totalPotentialSentences+len(reducedParagraphs[bk])\n",
    "\t\tscoreTuples.sort(key=lambda tup: tup[0])\n",
    "\t\tfinalTuples=list()\n",
    "\t\tk=0\n",
    "\t\ti=0\n",
    "\t\twhile i<len(scoreTuples):\n",
    "\t\t\tsenttups=scoreTuples[i:i+totalPotentialSentences]\n",
    "\t\t\tsenttups.sort(key=itemgetter(12,8),reverse=True)\n",
    "\t\t\tif senttups[0][8]>threshold:\n",
    "\t\t\t\tfinalTuples.append(senttups[0])\n",
    "\t\t\ti=i+totalPotentialSentences\n",
    "\t\t\tk=k+1\n",
    "\n",
    "\t\tfinalTuples.sort(key=lambda tup: tup[8])\n",
    "\n",
    "\t\t# Extracting tuples that have large differences in syntactic and semantic values and atleast one of them is greater than 0.8\n",
    "\n",
    "\n",
    "\t\treturn finalTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacyExtract(textPara,reducedBooks):\n",
    "    spacyTextPara=[]\n",
    "    for para in textPara:\n",
    "        spacyTextPara.append(sp(para))\n",
    "        \n",
    "    spacyBooksPara=dict()\n",
    "    for book in booksList:\n",
    "        l=[]\n",
    "        for para in reducedBooks[book]:\n",
    "            l.append(sp(para))\n",
    "        spacyBooksPara[book]=l\n",
    "    return spacyTextPara,spacyBooksPara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jacardNouns(sent1,sent2):\n",
    "    \n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN'))]\n",
    "    # b=sp(sent2)\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN'))]\n",
    "\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "'''\n",
    "Returns the jaccard index of verbs in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardVerbs(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "    \n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if token.pos_ == 'VERB']\n",
    "    # b=sp(sent2)\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if token.pos_ == 'VERB']\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "'''\n",
    "Returns the jaccard index of adjectives in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardAdj(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if token.pos_ == 'ADJ']\n",
    "    # b=sp(sent2,disable=['parser','ner','textcat','entity'])\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if token.pos_ == 'ADJ']\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef nounBasedRanking(finalTuples,textPara,reducedParagraphs):\n",
    "\t\tnewTuples=list()\n",
    "\t\tfor tup in finalTuples:\n",
    "\t\t\toriginalSent=textPara[tup[0]]\n",
    "\t\t\trefSent=reducedParagraphs[tup[1]][tup[2]]\n",
    "\t\t\tnounScore=jacardNouns(originalSent,refSent)\n",
    "\t\t\tverbScore=jacardVerbs(originalSent,refSent)\n",
    "\t\t\tadjScore=jacardAdj(originalSent,refSent)\n",
    "\t\t\tnewTuples.append(tup+(nounScore,verbScore,adjScore))\n",
    "\t\tnewTuples.sort(key=itemgetter(13,8),reverse=True)\n",
    "\t\treturn newTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef writeOutput(newTuples,textPara,reducedParagraphs,file):\n",
    "\t\tf=open(file,'w')\n",
    "\t\ti=1\n",
    "\t\tlines=list()\n",
    "\t\tfor t in newTuples:\n",
    "\t\t\tj=str(i)\n",
    "\t\t\tlines.append('Pairing: '+j)\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('New Sentence: '+textPara[t[0]])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Reference: \\n'+reducedParagraphs[t[1]][t[2]])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Similar Sentence is from: '+str(t[1]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Syntactic Score: '+str(t[3]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Syntactic Similarity without tokens: '+str(t[11]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic Score: '+str(t[4]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic Score without stopwords: '+str(t[5]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('LCS Length: '+str(t[9]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('LCS: '+t[10])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common nouns: '+str(t[13]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common verbs: '+str(t[14]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common adjectives: '+str(t[15]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic similarity nouns: '+str(t[6]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic similarity verbs: '+str(t[7]))\n",
    "\t\t\tlines.append('\\n\\n')\n",
    "\t\t\ti=i+1\n",
    "\t\tf.writelines(lines)\n",
    "\t\treturn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nietzsche n1-lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/n1-lim/new/Nietzsche1.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>11, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/n1-lim/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "textPara=splitNewPara(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/n1-lim/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacyTextPara,spacyBooksPara=spacyExtract(textPara,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/n1-lim/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=finalFiltering(scoreTuples,reducedBooks,0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTuples=nounBasedRanking(finalTuples,spacyTextPara,spacyBooksPara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(newTuples,textPara,reducedBooks,'../output/n1-lim/nounPairsReduced2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=StanfordCoreNLP('/home/users2/mehrotsh/scripts/packages/stanford-corenlp-full-2018-02-27/',memory='8g',lang='de',timeout=1000000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de=nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/temp/new/newTestament.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/temp/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/temp/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=[]\n",
    "for tup in scoreTuples:\n",
    "    if tup[8]>0.8:\n",
    "        f.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19,\n",
       "  'isaiah',\n",
       "  1,\n",
       "  0.7851042548788925,\n",
       "  0.8473451137542725,\n",
       "  0.8214048743247986,\n",
       "  0.6591166257858276,\n",
       "  0.7765181660652161,\n",
       "  0.8032545646018455,\n",
       "  4,\n",
       "  'lord saying unto thee ',\n",
       "  0.8106461307346255,\n",
       "  1),\n",
       " (19,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.8086221593855051,\n",
       "  0.8733952045440674,\n",
       "  0.8457973599433899,\n",
       "  0.7078565955162048,\n",
       "  0.7686445116996765,\n",
       "  0.8272097596644474,\n",
       "  6,\n",
       "  'man behold lord saying unto thee ',\n",
       "  0.8407560951849827,\n",
       "  1),\n",
       " (19,\n",
       "  'jeremiah-1',\n",
       "  8,\n",
       "  0.7902905014147295,\n",
       "  0.861167848110199,\n",
       "  0.834205687046051,\n",
       "  0.7346094846725464,\n",
       "  0.7394408583641052,\n",
       "  0.8122480942303902,\n",
       "  4,\n",
       "  'lord saying unto thee ',\n",
       "  0.8249963123247723,\n",
       "  1),\n",
       " (19,\n",
       "  'jeremiah-1',\n",
       "  10,\n",
       "  0.8012914358059989,\n",
       "  0.8657531142234802,\n",
       "  0.8299949169158936,\n",
       "  0.6432461738586426,\n",
       "  0.7651503682136536,\n",
       "  0.8156431763609462,\n",
       "  4,\n",
       "  'came make thee thy ',\n",
       "  0.8434118035955296,\n",
       "  1),\n",
       " (19,\n",
       "  'jeremiah-1',\n",
       "  11,\n",
       "  0.7793291897859772,\n",
       "  0.8596692085266113,\n",
       "  0.8340336680412292,\n",
       "  0.7044985294342041,\n",
       "  0.7686419486999512,\n",
       "  0.8066814289136033,\n",
       "  7,\n",
       "  'came unto saying thou take thee wife ',\n",
       "  0.8169509541252478,\n",
       "  1),\n",
       " (20,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.7834362320295033,\n",
       "  0.8939644694328308,\n",
       "  0.8647235631942749,\n",
       "  0.7282604575157166,\n",
       "  0.7543252110481262,\n",
       "  0.824079897611889,\n",
       "  6,\n",
       "  'man behold lord saying unto thee ',\n",
       "  0.821909469458477,\n",
       "  1),\n",
       " (20,\n",
       "  'jeremiah-1',\n",
       "  8,\n",
       "  0.762195948622308,\n",
       "  0.8795015215873718,\n",
       "  0.8493766188621521,\n",
       "  0.7454626560211182,\n",
       "  0.7070990800857544,\n",
       "  0.80578628374223,\n",
       "  4,\n",
       "  'lord saying unto thee ',\n",
       "  0.7965114577998914,\n",
       "  1),\n",
       " (20,\n",
       "  'jeremiah-1',\n",
       "  10,\n",
       "  0.7649062197367935,\n",
       "  0.8845135569572449,\n",
       "  0.8575365543365479,\n",
       "  0.672707200050354,\n",
       "  0.7235517501831055,\n",
       "  0.8112213870366707,\n",
       "  3,\n",
       "  'make thee thy ',\n",
       "  0.8072601355558627,\n",
       "  1),\n",
       " (20,\n",
       "  'jeremiah-1',\n",
       "  11,\n",
       "  0.7481076954578294,\n",
       "  0.8802469372749329,\n",
       "  0.8552893400192261,\n",
       "  0.7107077240943909,\n",
       "  0.7237933874130249,\n",
       "  0.8016985177385277,\n",
       "  7,\n",
       "  'lord unto saying thou take thee wife ',\n",
       "  0.7889315275655054,\n",
       "  1),\n",
       " (21,\n",
       "  'isaiah',\n",
       "  1,\n",
       "  0.7451072620560447,\n",
       "  0.9002768993377686,\n",
       "  0.8868061900138855,\n",
       "  0.8146073222160339,\n",
       "  0.8176098465919495,\n",
       "  0.8159567260349652,\n",
       "  4,\n",
       "  'lord saying unto thee ',\n",
       "  0.7897130188335573,\n",
       "  1),\n",
       " (21,\n",
       "  'isaiah',\n",
       "  2,\n",
       "  0.7454536558298268,\n",
       "  0.8996133208274841,\n",
       "  0.8775494694709778,\n",
       "  0.7315778136253357,\n",
       "  0.7783743739128113,\n",
       "  0.8115015626504023,\n",
       "  5,\n",
       "  'lord thou take thee forth ',\n",
       "  0.7699193817161176,\n",
       "  1),\n",
       " (21,\n",
       "  'isaiah',\n",
       "  5,\n",
       "  0.764353063411696,\n",
       "  0.882875919342041,\n",
       "  0.8526406288146973,\n",
       "  0.6859091520309448,\n",
       "  0.8141582608222961,\n",
       "  0.8084968461131966,\n",
       "  4,\n",
       "  '21 shall forth thou ',\n",
       "  0.7976227066145211,\n",
       "  0),\n",
       " (21,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.7825982843682167,\n",
       "  0.8988550305366516,\n",
       "  0.8803250193595886,\n",
       "  0.7931347489356995,\n",
       "  0.8268411755561829,\n",
       "  0.8314616518639026,\n",
       "  5,\n",
       "  'behold lord saying unto thee ',\n",
       "  0.8210197101924475,\n",
       "  1),\n",
       " (21,\n",
       "  'jeremiah-1',\n",
       "  8,\n",
       "  0.7591553843507147,\n",
       "  0.8962919116020203,\n",
       "  0.8710097670555115,\n",
       "  0.8069331049919128,\n",
       "  0.7949486970901489,\n",
       "  0.815082575703113,\n",
       "  5,\n",
       "  'lord saying unto thee shall ',\n",
       "  0.8038209011546659,\n",
       "  1),\n",
       " (21,\n",
       "  'jeremiah-1',\n",
       "  9,\n",
       "  0.7554219878622938,\n",
       "  0.9112333655357361,\n",
       "  0.8844099640846252,\n",
       "  0.7774149179458618,\n",
       "  0.8472694158554077,\n",
       "  0.8199159759734596,\n",
       "  6,\n",
       "  'unto thee thy 21 shall forth ',\n",
       "  0.7938309108780284,\n",
       "  1),\n",
       " (21,\n",
       "  'jeremiah-1',\n",
       "  10,\n",
       "  0.7530002656787886,\n",
       "  0.910626232624054,\n",
       "  0.8885387182235718,\n",
       "  0.7311974167823792,\n",
       "  0.7851994633674622,\n",
       "  0.8207694919511802,\n",
       "  3,\n",
       "  'behold thee thy ',\n",
       "  0.7885624722455244,\n",
       "  1),\n",
       " (21,\n",
       "  'jeremiah-1',\n",
       "  11,\n",
       "  0.7468572193378576,\n",
       "  0.9000637531280518,\n",
       "  0.8850211501121521,\n",
       "  0.8548596501350403,\n",
       "  0.7390403747558594,\n",
       "  0.8159391847250048,\n",
       "  8,\n",
       "  'lord unto saying thou take thee wife thou ',\n",
       "  0.7986695701763571,\n",
       "  1),\n",
       " (23,\n",
       "  'isaiah',\n",
       "  2,\n",
       "  0.7170463062376626,\n",
       "  0.8920150995254517,\n",
       "  0.8920813798904419,\n",
       "  0.7835615873336792,\n",
       "  0.7799092531204224,\n",
       "  0.8045638430640523,\n",
       "  4,\n",
       "  'shall thou call shall ',\n",
       "  0.7759772957599562,\n",
       "  1),\n",
       " (23,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.7366993837093062,\n",
       "  0.9042367935180664,\n",
       "  0.8782974481582642,\n",
       "  0.824806272983551,\n",
       "  0.8386931419372559,\n",
       "  0.8074984159337852,\n",
       "  5,\n",
       "  'shall shall people lord saying ',\n",
       "  0.785991172127172,\n",
       "  1),\n",
       " (23,\n",
       "  'jeremiah-1',\n",
       "  9,\n",
       "  0.735162910719779,\n",
       "  0.9070204496383667,\n",
       "  0.8700037002563477,\n",
       "  0.7719591856002808,\n",
       "  0.8983557224273682,\n",
       "  0.8025833054880633,\n",
       "  4,\n",
       "  '21 shall forth shall ',\n",
       "  0.7929247835125189,\n",
       "  1),\n",
       " (24,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.7496908150696461,\n",
       "  0.8702905774116516,\n",
       "  0.8641183376312256,\n",
       "  0.8127021193504333,\n",
       "  0.8106606602668762,\n",
       "  0.8069045763504359,\n",
       "  5,\n",
       "  'shall people lord saying god ',\n",
       "  0.8076238028259974,\n",
       "  2),\n",
       " (24,\n",
       "  'jeremiah-1',\n",
       "  9,\n",
       "  0.7448192536875269,\n",
       "  0.8960400223731995,\n",
       "  0.8707177042961121,\n",
       "  0.7905417084693909,\n",
       "  0.9309395551681519,\n",
       "  0.8077684789918195,\n",
       "  6,\n",
       "  'shall 22 behold shall forth shall ',\n",
       "  0.8081031834387343,\n",
       "  1),\n",
       " (24,\n",
       "  'jeremiah-1',\n",
       "  12,\n",
       "  0.7136609710408154,\n",
       "  0.9143249988555908,\n",
       "  0.9000387191772461,\n",
       "  0.7707833051681519,\n",
       "  0.9152401685714722,\n",
       "  0.8068498451090307,\n",
       "  4,\n",
       "  'shall lord behold shall ',\n",
       "  0.7710316150204004,\n",
       "  1),\n",
       " (25,\n",
       "  'isaiah',\n",
       "  0,\n",
       "  0.7934179725327397,\n",
       "  0.9087806940078735,\n",
       "  0.8369399309158325,\n",
       "  0.7815638184547424,\n",
       "  0.9106940031051636,\n",
       "  0.8151789517242861,\n",
       "  5,\n",
       "  'spoken shall god us lord ',\n",
       "  0.8554292074657792,\n",
       "  2),\n",
       " (25,\n",
       "  'isaiah',\n",
       "  2,\n",
       "  0.777308364856714,\n",
       "  0.8317497372627258,\n",
       "  0.8343471884727478,\n",
       "  0.679547131061554,\n",
       "  0.7653993964195251,\n",
       "  0.8058277766647308,\n",
       "  4,\n",
       "  'lord shall shall forth ',\n",
       "  0.8090525735233061,\n",
       "  1),\n",
       " (25,\n",
       "  'jeremiah-1',\n",
       "  1,\n",
       "  0.7648575314403209,\n",
       "  0.8732700347900391,\n",
       "  0.8478220701217651,\n",
       "  0.8527916073799133,\n",
       "  0.6946538090705872,\n",
       "  0.806339800781043,\n",
       "  3,\n",
       "  'spoken lord god ',\n",
       "  0.812020935706758,\n",
       "  2),\n",
       " (25,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.8174521402917858,\n",
       "  0.8842651844024658,\n",
       "  0.8749596476554871,\n",
       "  0.8017287254333496,\n",
       "  0.8195533156394958,\n",
       "  0.8462058939736364,\n",
       "  4,\n",
       "  'lord saying god unto ',\n",
       "  0.8663047079966336,\n",
       "  2),\n",
       " (25,\n",
       "  'jeremiah-1',\n",
       "  3,\n",
       "  0.7610738815996362,\n",
       "  0.874071478843689,\n",
       "  0.850290060043335,\n",
       "  0.743589460849762,\n",
       "  0.7092738151550293,\n",
       "  0.8056819708214855,\n",
       "  5,\n",
       "  'lord prophet saying lord unto ',\n",
       "  0.7962713613593497,\n",
       "  1),\n",
       " (25,\n",
       "  'jeremiah-1',\n",
       "  8,\n",
       "  0.7978842693210095,\n",
       "  0.8690304160118103,\n",
       "  0.8573224544525146,\n",
       "  0.798334002494812,\n",
       "  0.781928539276123,\n",
       "  0.8276033618867621,\n",
       "  4,\n",
       "  'spoken lord bring shall ',\n",
       "  0.8414139692834458,\n",
       "  2),\n",
       " (25,\n",
       "  'jeremiah-1',\n",
       "  9,\n",
       "  0.8032325221705157,\n",
       "  0.903761625289917,\n",
       "  0.8715657591819763,\n",
       "  0.7952262759208679,\n",
       "  0.9074073433876038,\n",
       "  0.837399140676246,\n",
       "  5,\n",
       "  '22 behold shall forth shall ',\n",
       "  0.8490479172666844,\n",
       "  1),\n",
       " (25,\n",
       "  'jeremiah-1',\n",
       "  10,\n",
       "  0.7872314314216103,\n",
       "  0.8470038175582886,\n",
       "  0.8476088643074036,\n",
       "  0.7857961654663086,\n",
       "  0.725309431552887,\n",
       "  0.8174201478645069,\n",
       "  3,\n",
       "  'lord name lord ',\n",
       "  0.84811953355887,\n",
       "  2),\n",
       " (25,\n",
       "  'jeremiah-1',\n",
       "  12,\n",
       "  0.7626506553579286,\n",
       "  0.9067121148109436,\n",
       "  0.8799005150794983,\n",
       "  0.7738303542137146,\n",
       "  0.8418777585029602,\n",
       "  0.8212755852187135,\n",
       "  4,\n",
       "  'shall shall shall lord ',\n",
       "  0.8100212553686645,\n",
       "  1),\n",
       " (26,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.7921068772166334,\n",
       "  0.8619344830513,\n",
       "  0.8213949799537659,\n",
       "  0.7189396023750305,\n",
       "  0.8260936737060547,\n",
       "  0.8067509285851997,\n",
       "  4,\n",
       "  'shall shall god unto ',\n",
       "  0.8329707898103297,\n",
       "  2),\n",
       " (26,\n",
       "  'jeremiah-1',\n",
       "  9,\n",
       "  0.7798335861473302,\n",
       "  0.9002827405929565,\n",
       "  0.8565218448638916,\n",
       "  0.7521727681159973,\n",
       "  0.9201480150222778,\n",
       "  0.8181777155056109,\n",
       "  6,\n",
       "  'shall shall forth shall brought forth ',\n",
       "  0.821219039291732,\n",
       "  1),\n",
       " (26,\n",
       "  'jeremiah-1',\n",
       "  10,\n",
       "  0.7840290377358222,\n",
       "  0.8525850176811218,\n",
       "  0.8216514587402344,\n",
       "  0.7220669388771057,\n",
       "  0.747086226940155,\n",
       "  0.8028402482380284,\n",
       "  3,\n",
       "  'forth name lord ',\n",
       "  0.8454925733691616,\n",
       "  2),\n",
       " (44,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.7915774107616835,\n",
       "  0.8524597883224487,\n",
       "  0.814522385597229,\n",
       "  0.7433168888092041,\n",
       "  0.7067157626152039,\n",
       "  0.8030498981794563,\n",
       "  4,\n",
       "  'behold lord saying thee ',\n",
       "  0.8346891936579113,\n",
       "  2),\n",
       " (44,\n",
       "  'jeremiah-1',\n",
       "  8,\n",
       "  0.789908477566097,\n",
       "  0.8493094444274902,\n",
       "  0.8146056532859802,\n",
       "  0.7449852824211121,\n",
       "  0.7281128764152527,\n",
       "  0.8022570654260386,\n",
       "  3,\n",
       "  'unto lord bring ',\n",
       "  0.8342489401569768,\n",
       "  2),\n",
       " (50,\n",
       "  'jeremiah-1',\n",
       "  2,\n",
       "  0.8141767245722316,\n",
       "  0.8286003470420837,\n",
       "  0.7987337708473206,\n",
       "  0.7222843766212463,\n",
       "  0.737365186214447,\n",
       "  0.8064552477097761,\n",
       "  3,\n",
       "  'lord saying spoken ',\n",
       "  0.8445560304487207,\n",
       "  1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n1-sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential='../data/n3-lim/potential/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/n3-lim/new/Nietzsche1.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>11, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamann_denkwuerdigkeiten_1759.txt\n"
     ]
    }
   ],
   "source": [
    "books=dict()\n",
    "for file in booksList:\n",
    "    print(file)\n",
    "    candidate=open(potential+file)\n",
    "    rawtext=candidate.read()\n",
    "    candidate.close()\n",
    "    rawtext = strip_headers(rawtext).strip()\n",
    "    candidate=rawtext.replace('\\n',' ')\n",
    "    candidate=rawtext.replace(':','. ')\n",
    "    candidate=sent_tokenize(candidate)\n",
    "    candidate = list(filter(lambda x: len(x)>11, candidate))\n",
    "    books[file]=candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/n3-lim-sent//reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/n3-lim-sent/orderedTuples.pickle\",\"rb\")\n",
    "orderedTuples=pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalOutput=list()\n",
    "for tup in orderedTuples:\n",
    "    output=list()\n",
    "    \n",
    "    originalSent=tup[0]\n",
    "    refBook=tup[1]\n",
    "    refSentReduced=tup[2]\n",
    "    ref=reducedBooks[refBook][refSentReduced]\n",
    "    refSent=books[refBook].index(ref)\n",
    "    bk=books[refBook]\n",
    "    \n",
    "    if originalSent==0:\n",
    "        original=text[originalSent:originalSent+2]\n",
    "    if originalSent==len(text)-1:\n",
    "        original=text[originalSent-2:originalSent+1]\n",
    "    else:\n",
    "        original=text[originalSent-1:originalSent+2]\n",
    "    \n",
    "    \n",
    "    if refSent==0:\n",
    "        reference=bk[refSent:refSent+2]\n",
    "    if refSent==len(bk)-1:\n",
    "        reference=bk[refSent-2:refSent+1]\n",
    "    else:\n",
    "        reference=bk[refSent-1:refSent+2]\n",
    "\n",
    "    original=' '.join(original)\n",
    "    reference=' '.join(reference)\n",
    "    \n",
    "    finalOutput.append((original,reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=list()\n",
    "i=0\n",
    "for output in finalOutput:\n",
    "    j=str(i+1)\n",
    "    lines.append(j)\n",
    "    lines.append('\\n')\n",
    "    lines.append('New Sentence: ')\n",
    "    lines.append(output[0])\n",
    "    lines.append('\\n')\n",
    "    lines.append('Reference Sentence: ')\n",
    "    lines.append(output[1])\n",
    "    lines.append('\\n\\n')\n",
    "    lines.append('\\n')\n",
    "    lines.append('Syntactic Score: '+str(orderedTuples[i][3]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('Syntactic Similarity without tokens: '+str(orderedTuples[i][11]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('Semantic Score: '+str(orderedTuples[i][4]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('Semantic Score without stopwords: '+str(orderedTuples[i][5]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('LCS Length: '+str(orderedTuples[i][9]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('LCS: '+orderedTuples[i][10])\n",
    "    lines.append('\\n')\n",
    "    lines.append('Jaccard of common nouns: '+str(orderedTuples[i][13]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('Jaccard of common verbs: '+str(orderedTuples[i][14]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('Jaccard of common adjectives: '+str(orderedTuples[i][15]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('Semantic similarity nouns: '+str(orderedTuples[i][6]))\n",
    "    lines.append('\\n')\n",
    "    lines.append('Semantic similarity verbs: '+str(orderedTuples[i][7]))\n",
    "    lines.append('\\n\\n')\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../output/n3-lim-sent/noun2','a+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python personal",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
