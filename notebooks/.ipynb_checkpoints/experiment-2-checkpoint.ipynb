{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring results from various data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import bisect\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import os\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "from nltk.tree import *\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import nltk.stem.snowball\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "from nltk.draw.tree import TreeView\n",
    "from fuzzywuzzy import fuzz\n",
    "from multiprocessing import Pool\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import wordnet \n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(): \n",
    "    return defaultdict(tree)\n",
    "\n",
    "\n",
    "def _leadingSpaces_(target):\n",
    "    return len(target) - len(target.lstrip())\n",
    "\n",
    "def _findParent_(curIndent, parid, treeRef):\n",
    "    tmpid = parid\n",
    "    while (curIndent <= treeRef[tmpid]['indent']):\n",
    "        tmpid = treeRef[tmpid]['parid']\n",
    "    return tmpid\n",
    "\n",
    "\n",
    "def generateTree(rawTokens, treeRef):\n",
    "\n",
    "    # (token\n",
    "    REGEX_OPEN = r\"^\\s*\\(([a-zA-Z0-9_']*)\\s*$\"\n",
    "    # (token (tok1 tok2) (tok3 tok4) .... (tokx toky))\n",
    "    REGEX_COMP = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*((?:[(]([a-zA-Z0-9_;.,?'!]+)\\s*([a-zA-Z0-9_;\\.,?!']+)[)]\\s*)+)\"    \n",
    "    # (, ,) as stand-alone. Used for match() not search()\n",
    "    REGEX_PUNC = r\"^\\s*\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "    # (tok1 tok2) as stand-alone\n",
    "    REGEX_SOLO_PAIR = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*([a-zA-Z0-9_']+)\\)\"\n",
    "    # (tok1 tok2) used in search()\n",
    "    REGEX_ISOL_IN_COMP = r\"\\(([a-zA-Z0-9_;.,?!']+)\\s*([a-zA-Z0-9_;.,?!']+)\\)\"\n",
    "    # (punc punc) used in search()\n",
    "    REGEX_PUNC_SOLO = r\"\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "   \n",
    "    treeRef[len(treeRef)] = {'curid':0, \n",
    "                             'parid':-1, \n",
    "                             'posOrTok':'ROOT', \n",
    "                             'indent':0,\n",
    "                            'children':[],\n",
    "                            'childrenTok':[]}\n",
    "    ID_CTR = 1\n",
    "    \n",
    "    for tok in rawTokens[1:]:\n",
    "        \n",
    "        curIndent = _leadingSpaces_(tok) \n",
    "        parid = _findParent_(curIndent, ID_CTR-1, treeRef)\n",
    "        \n",
    "        # CHECK FOR COMPOSITE TOKENS\n",
    "        checkChild = re.match(REGEX_COMP, tok)\n",
    "        if (checkChild):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkChild.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            upCTR = ID_CTR\n",
    "            ID_CTR += 1\n",
    "            \n",
    "            subCheck = re.sub(REGEX_PUNC_SOLO,'',checkChild.group(2))\n",
    "            subs = re.findall(REGEX_ISOL_IN_COMP, subCheck) \n",
    "            for ch in subs:\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':upCTR, \n",
    "                                   'posOrTok':ch[0], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':ID_CTR-1, \n",
    "                                   'posOrTok':ch[1], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "            continue\n",
    "           \n",
    "\n",
    "            \n",
    "        checkSingle = re.match(REGEX_SOLO_PAIR, tok)\n",
    "        if (checkSingle):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkSingle.group(1), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':ID_CTR-1, \n",
    "                               'posOrTok':checkSingle.group(2), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        checkPunc = re.match(REGEX_PUNC, tok)\n",
    "        if (checkPunc): # ignore punctuation\n",
    "            continue\n",
    "\n",
    "        checkMatch = re.match(REGEX_OPEN, tok)\n",
    "        if (checkMatch):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkMatch.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "\n",
    "    return\n",
    "            \n",
    "\n",
    "def flipTree(treeRef):\n",
    "    # Pass 1 fill in children\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            bisect.insort(treeRef[v['parid']]['children'], k)\n",
    "    # Pass 2 map children to tokens\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            treeRef[k]['childrenTok'] = [treeRef[ch]['posOrTok'] for ch in treeRef[k]['children']]\n",
    "    treeRef[0]['childrenTok'] = treeRef[1]['posOrTok']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isLeaf_(tree, parentNode):\n",
    "    return (len(tree[parentNode]['children']) == 0)\n",
    "\n",
    "def _isPreterminal_(tree, parentNode):\n",
    "    for idx in tree[parentNode]['children']:\n",
    "        if not _isLeaf_(tree, idx):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel\n",
    "'''\n",
    "\n",
    "def _cdHelper_(tree1, tree2, node1, node2, store, lam, SST_ON):\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "        # same children tokens\n",
    "        if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "            # Check if both nodes are pre-terminal\n",
    "            if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "                store[node1, node2] = lam\n",
    "                return\n",
    "            # Not pre-terminal. Recurse among the children of both token trees.\n",
    "            else:\n",
    "                nChildren = len(tree1[node1]['children'])\n",
    "\n",
    "                runningTotal = None\n",
    "                for idx in range(nChildren):\n",
    "                     # index ->  node_id\n",
    "                    tmp_n1 = tree1[node1]['children'][idx]\n",
    "                    tmp_n2 = tree2[node2]['children'][idx]\n",
    "                    # Recursively run helper\n",
    "                    _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON)\n",
    "                    # Set the initial value for the layer. Else multiplicative product.\n",
    "                    if (runningTotal == None):\n",
    "                        runningTotal = SST_ON + store[tmp_n1, tmp_n2]\n",
    "                    else:\n",
    "                        runningTotal *= (SST_ON + store[tmp_n1, tmp_n2])\n",
    "\n",
    "                store[node1, node2] = lam * runningTotal\n",
    "                return\n",
    "        else:\n",
    "            store[node1, node2] = 0\n",
    "    else: # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "\n",
    "def _cdKernel_(tree1, tree2, lam, SST_ON):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def CollinsDuffy(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON):\n",
    "    raw_score = _cdKernel_(tree1, tree2, lam, SST_ON)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _cdKernel_(tree1, tree1, lam, SST_ON)\n",
    "        t2_score = _cdKernel_(tree2, tree2, lam, SST_ON)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Implementation of the Partial Tree (PT) Kernel from:\n",
    "\"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees\"\n",
    "by Alessandro Moschitti\n",
    "'''\n",
    "\n",
    "'''\n",
    "The delta function is stolen from the Collins-Duffy kernel\n",
    "'''\n",
    "\n",
    "def _deltaP_(tree1, tree2, seq1, seq2, store, lam, mu, p):\n",
    "\n",
    "#     # Enumerate subsequences of length p+1 for each child set\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # generate delta(a,b)\n",
    "        _delta_(tree1, tree2, seq1[-1], seq2[-1], store, lam, mu)\n",
    "        if store[seq1[-1], seq2[-1]] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            runningTot = 0\n",
    "            for i in range(p-1, len(seq1)-1):\n",
    "                for r in range(p-1, len(seq2)-1):\n",
    "                    scaleFactor = pow(lam, len(seq1[:-1])-i+len(seq2[:-1])-r)\n",
    "                    dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-1)\n",
    "                    runningTot += (scaleFactor * dp)\n",
    "            return runningTot\n",
    "\n",
    "def _delta_(tree1, tree2, node1, node2, store, lam, mu):\n",
    "\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "\n",
    "        if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "            if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "                store[node1, node2] = lam\n",
    "            else:\n",
    "                store[node1, node2] = 0\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # establishes p_max\n",
    "            childmin = min(len(tree1[node1]['children']), len(tree2[node2]['children']))\n",
    "            deltaTot = 0\n",
    "            for p in range(1,childmin+1):\n",
    "                # compute delta_p\n",
    "                deltaTot += _deltaP_(tree1, tree2,\n",
    "                                     tree1[node1]['children'],\n",
    "                                     tree2[node2]['children'], store, lam, mu, p)\n",
    "\n",
    "            store[node1, node2] = mu * (pow(lam,2) + deltaTot)\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "def _ptKernel_(tree1, tree2, lam, mu):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _delta_(tree1, tree2, i, j, store, lam, mu)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def MoschittiPT(tree1, tree2, lam, mu, NORMALIZE_FLAG):\n",
    "    raw_score = _ptKernel_(tree1, tree2, lam, mu)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _ptKernel_(tree1, tree1, lam, mu)\n",
    "        t2_score = _ptKernel_(tree2, tree2, lam, mu)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jacardNouns(sent1,sent2):\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent1)):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent2)):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "#     print(nouns1)\n",
    "#     print(nouns2)\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "def jacardVerbs(sent1,sent2):\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent1)):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent2)):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "#     print(nouns1)\n",
    "#     print(nouns2)\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "def jacardAdj(sent1,sent2):\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent1)):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(word_tokenize(sent2)):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "#     print(nouns1)\n",
    "#     print(nouns2)\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeTuples(scoreTuples):\n",
    "    newTupes=list()\n",
    "    for t in scoreTuples:\n",
    "        newTup=(t[0],t[1],t[2],t[3],t[4],t[5],t[6],t[7],(t[3]+t[5])/2,t[9],t[10],t[11])\n",
    "        newTupes.append(newTup)\n",
    "    return newTupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef finalFiltering(scoreTuples,reducedBooks,threshold=0.85):\n",
    "\t\ttotalPotentialSentences=0\n",
    "\t\tfor bk in booksList:\n",
    "\t\t\ttotalPotentialSentences=totalPotentialSentences+len(reducedBooks[bk])\n",
    "\t\tscoreTuples.sort(key=lambda tup: tup[0])\n",
    "\t\tfinalTuples=list()\n",
    "\t\tk=0\n",
    "\t\ti=0\n",
    "\t\twhile i<len(scoreTuples):\n",
    "\t\t\tsenttups=scoreTuples[i:i+totalPotentialSentences]\n",
    "\t\t\tsenttups.sort(key=lambda tup: tup[8],reverse=True)\n",
    "\t\t\tif senttups[0][8]>threshold:\n",
    "\t\t\t\tfinalTuples.append(senttups[0])\n",
    "# \t\t\t\tfinalTuples.append(senttups[1])\n",
    "# \t\t\t\tfinalTuples.append(senttups[2])\n",
    "\t\t\ti=i+totalPotentialSentences\n",
    "\t\t\tk=k+1\n",
    "\n",
    "\t\tfinalTuples.sort(key=lambda tup: tup[8])\n",
    "\n",
    "\t\tdiffTuples=list()\n",
    "\t\tfor tup in scoreTuples:\n",
    "\t\t\tif (tup[3]>0.8 and abs(tup[3]-tup[4])>=0.12) or (tup[4]>0.8 and abs(tup[3]-tup[4])>=0.12):\n",
    "\t\t\t\tdiffTuples.append(tup)\n",
    "\n",
    "\t\treturn finalTuples,diffTuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef nounBasedRanking(finalTuples,text,reducedBooks):\n",
    "\t\tnewTuples=list()\n",
    "\t\tfor tup in finalTuples:\n",
    "\t\t\toriginalSent=text[tup[0]]\n",
    "\t\t\trefSent=reducedBooks[tup[1]][tup[2]]\n",
    "\t\t\tnounScore=jacardNouns(originalSent,refSent)\n",
    "\t\t\tverbScore=jacardVerbs(originalSent,refSent)\n",
    "\t\t\tadjScore=jacardAdj(originalSent,refSent)\n",
    "\t\t\tnewTuples.append(tup+(nounScore,verbScore,adjScore))\n",
    "\t\tnewTuples.sort(key=itemgetter(12,8),reverse=True)\n",
    "\t\treturn newTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef writeOutput(newTuples,text,reducedBooks,fileName):\n",
    "\t\tf=open(fileName,'w')\n",
    "\t\ti=1\n",
    "\t\tlines=list()\n",
    "\t\tfor t in newTuples:\n",
    "\t\t\tj=str(i)\n",
    "\t\t\tlines.append('Pairing: '+j)\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('New Sentence: '+text[t[0]])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Reference: \\n'+reducedBooks[t[1]][t[2]])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Similar Sentence is from: '+str(t[1]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Syntactic Score: '+str(t[3]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Syntactic Similarity without tokens: '+str(t[11]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic Score: '+str(t[4]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic Score without stopwords: '+str(t[5]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('LCS Length: '+str(t[9]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('LCS: '+t[10])\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common nouns: '+str(t[12]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common verbs: '+str(t[13]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Jaccard of common adjectives: '+str(t[14]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic similarity nouns: '+str(t[6]))\n",
    "\t\t\tlines.append('\\n')\n",
    "\t\t\tlines.append('Semantic similarity verbs: '+str(t[7]))\n",
    "\t\t\tlines.append('\\n\\n')\n",
    "\t\t\ti=i+1\n",
    "\t\tf.writelines(lines)\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe/new/poe-pit-110.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=changeTuples(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe/'+'nounSortedSentencePairs5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-2/new/poe-colloquy-675.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-2/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-2/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-2/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401650"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-2/'+'nounSortedSentencePairs5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-3/new/poe-black-670.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-3/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132488"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-3/'+'nounSortedSentencePairs4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-4/new/poe-power-699.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-4/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-4/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-4/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675630"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-4/'+'nounSortedSentencePairs2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-5/new/poe-domain-679.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-5/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-5/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-5/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1719103"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-5/'+'nounSortedSentencePairs2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poe-3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"../data/poe-3/new/poe-black-670.txt\"\n",
    "testB=open(test)\n",
    "raw=testB.read()\n",
    "text = strip_headers(raw).strip()\n",
    "text=text.replace('\\n',' ')\n",
    "text=text.replace(':','. ')\n",
    "text=sent_tokenize(text)\n",
    "text = list(filter(lambda x: len(x)>5, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksList=os.listdir('../data/poe-3/potential/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3-2/reducedBooks.pickle\",\"rb\")\n",
    "reducedBooks = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_off = open(\"../output/poe-3-2/scoreTuples.pickle\",\"rb\")\n",
    "scoreTuples = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1342845"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scoreTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples,diffTuples=finalFiltering(scoreTuples,reducedBooks,0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTuples=nounBasedRanking(finalTuples,text,reducedBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(finalTuples,text,reducedBooks,'../output/poe-3-2/'+'nounSortedSentencePairs3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python personal",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
