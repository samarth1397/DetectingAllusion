{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded data and extracted pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from lxml import etree\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('../../Nachweise/'):\n",
    "    tree = ET.parse('../../Nachweise/'+file)\n",
    "    root=tree.getroot()\n",
    "    children=[]\n",
    "    for child in root:\n",
    "        children.append(child.tag)\n",
    "    if len(children)==4:\n",
    "        fileNames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames=sorted(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryOfReferences=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in fileNames:\n",
    "    text = open('../../Nachweise/'+file).read()\n",
    "    root = etree.fromstring(text)[1]\n",
    "    ni=''.join(root.itertext())\n",
    "    ni=ni.replace('\\t','')\n",
    "    ni=ni.replace('\\n','')\n",
    "    \n",
    "    root = etree.fromstring(text)[2]\n",
    "    cite=''.join(root.itertext())\n",
    "    cite=cite.replace('\\t','')\n",
    "    cite=cite.replace('\\n','')\n",
    "    \n",
    "    root = etree.fromstring(text)[3]\n",
    "    ref=[root.text]\n",
    "    for i in root.iterdescendants():\n",
    "        if i.tag=='fn':\n",
    "            break\n",
    "        if i.text!=None:\n",
    "            ref.append(i.text)\n",
    "        if i.tail!=None:\n",
    "            ref.append(i.tail)\n",
    "    ref=''.join(ref)\n",
    "    ref=ref.replace('\\t','')\n",
    "    ref=ref.replace('\\n','')\n",
    "    \n",
    "    dictionaryOfReferences[file]=[ni,cite,ref]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting metrics from pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import bisect\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import os\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "from nltk.tree import *\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import nltk.stem.snowball\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "from nltk.draw.tree import TreeView\n",
    "from fuzzywuzzy import fuzz\n",
    "from multiprocessing import Pool\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import wordnet \n",
    "from operator import itemgetter\n",
    "# from langdetect import detect\n",
    "from polyglot.detect import Detector\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "public='/home/users2/mehrotsh/scripts/packages/stanford-corenlp-full-2018-02-27/'\n",
    "personal='/home/samarth/stanford-corenlp-full-2018-02-27/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=StanfordCoreNLP('/home/users2/mehrotsh/scripts/packages/stanford-corenlp-full-2018-02-27/',memory='8g',lang='de',timeout=1000000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=spacy.load('de',disable=['parser','ner','textcat','entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(): \n",
    "    return defaultdict(tree)\n",
    "\n",
    "\n",
    "def _leadingSpaces_(target):\n",
    "    return len(target) - len(target.lstrip())\n",
    "\n",
    "def _findParent_(curIndent, parid, treeRef):\n",
    "    tmpid = parid\n",
    "    while (curIndent <= treeRef[tmpid]['indent']):\n",
    "        tmpid = treeRef[tmpid]['parid']\n",
    "    return tmpid\n",
    "\n",
    "\n",
    "def generateTree(rawTokens, treeRef):\n",
    "\n",
    "    # (token\n",
    "    REGEX_OPEN = r\"^\\s*\\(([a-zA-Z0-9_']*)\\s*$\"\n",
    "    # (token (tok1 tok2) (tok3 tok4) .... (tokx toky))\n",
    "    REGEX_COMP = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*((?:[(]([a-zA-Z0-9_;.,?'!]+)\\s*([a-zA-Z0-9_;\\.,?!']+)[)]\\s*)+)\"    \n",
    "    # (, ,) as stand-alone. Used for match() not search()\n",
    "    REGEX_PUNC = r\"^\\s*\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "    # (tok1 tok2) as stand-alone\n",
    "    REGEX_SOLO_PAIR = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*([a-zA-Z0-9_']+)\\)\"\n",
    "    # (tok1 tok2) used in search()\n",
    "    REGEX_ISOL_IN_COMP = r\"\\(([a-zA-Z0-9_;.,?!']+)\\s*([a-zA-Z0-9_;.,?!']+)\\)\"\n",
    "    # (punc punc) used in search()\n",
    "    REGEX_PUNC_SOLO = r\"\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "   \n",
    "    treeRef[len(treeRef)] = {'curid':0, \n",
    "                             'parid':-1, \n",
    "                             'posOrTok':'ROOT', \n",
    "                             'indent':0,\n",
    "                            'children':[],\n",
    "                            'childrenTok':[]}\n",
    "    ID_CTR = 1\n",
    "    \n",
    "    for tok in rawTokens[1:]:\n",
    "        \n",
    "        curIndent = _leadingSpaces_(tok) \n",
    "        parid = _findParent_(curIndent, ID_CTR-1, treeRef)\n",
    "        \n",
    "        # CHECK FOR COMPOSITE TOKENS\n",
    "        checkChild = re.match(REGEX_COMP, tok)\n",
    "        if (checkChild):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkChild.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            upCTR = ID_CTR\n",
    "            ID_CTR += 1\n",
    "            \n",
    "            subCheck = re.sub(REGEX_PUNC_SOLO,'',checkChild.group(2))\n",
    "            subs = re.findall(REGEX_ISOL_IN_COMP, subCheck) \n",
    "            for ch in subs:\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':upCTR, \n",
    "                                   'posOrTok':ch[0], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':ID_CTR-1, \n",
    "                                   'posOrTok':ch[1], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "            continue\n",
    "           \n",
    "\n",
    "            \n",
    "        checkSingle = re.match(REGEX_SOLO_PAIR, tok)\n",
    "        if (checkSingle):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkSingle.group(1), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':ID_CTR-1, \n",
    "                               'posOrTok':checkSingle.group(2), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        checkPunc = re.match(REGEX_PUNC, tok)\n",
    "        if (checkPunc): # ignore punctuation\n",
    "            continue\n",
    "\n",
    "        checkMatch = re.match(REGEX_OPEN, tok)\n",
    "        if (checkMatch):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkMatch.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "\n",
    "    return\n",
    "            \n",
    "\n",
    "def flipTree(treeRef):\n",
    "    # Pass 1 fill in children\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            bisect.insort(treeRef[v['parid']]['children'], k)\n",
    "    # Pass 2 map children to tokens\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            treeRef[k]['childrenTok'] = [treeRef[ch]['posOrTok'] for ch in treeRef[k]['children']]\n",
    "    treeRef[0]['childrenTok'] = treeRef[1]['posOrTok']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isLeaf_(tree, parentNode):\n",
    "    return (len(tree[parentNode]['children']) == 0)\n",
    "\n",
    "def _isPreterminal_(tree, parentNode):\n",
    "    for idx in tree[parentNode]['children']:\n",
    "        if not _isLeaf_(tree, idx):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel\n",
    "'''\n",
    "\n",
    "def _cdHelper_(tree1, tree2, node1, node2, store, lam, SST_ON):\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "        # same children tokens\n",
    "        if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "            # Check if both nodes are pre-terminal\n",
    "            if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "                store[node1, node2] = lam\n",
    "                return\n",
    "            # Not pre-terminal. Recurse among the children of both token trees.\n",
    "            else:\n",
    "                nChildren = len(tree1[node1]['children'])\n",
    "\n",
    "                runningTotal = None\n",
    "                for idx in range(nChildren):\n",
    "                     # index ->  node_id\n",
    "                    tmp_n1 = tree1[node1]['children'][idx]\n",
    "                    tmp_n2 = tree2[node2]['children'][idx]\n",
    "                    # Recursively run helper\n",
    "                    _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON)\n",
    "                    # Set the initial value for the layer. Else multiplicative product.\n",
    "                    if (runningTotal == None):\n",
    "                        runningTotal = SST_ON + store[tmp_n1, tmp_n2]\n",
    "                    else:\n",
    "                        runningTotal *= (SST_ON + store[tmp_n1, tmp_n2])\n",
    "\n",
    "                store[node1, node2] = lam * runningTotal\n",
    "                return\n",
    "        else:\n",
    "            store[node1, node2] = 0\n",
    "    else: # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "\n",
    "def _cdKernel_(tree1, tree2, lam, SST_ON):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def CollinsDuffy(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON):\n",
    "    raw_score = _cdKernel_(tree1, tree2, lam, SST_ON)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _cdKernel_(tree1, tree1, lam, SST_ON)\n",
    "        t2_score = _cdKernel_(tree2, tree2, lam, SST_ON)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Implementation of the Partial Tree (PT) Kernel from:\n",
    "\"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees\"\n",
    "by Alessandro Moschitti\n",
    "'''\n",
    "\n",
    "'''\n",
    "The delta function is stolen from the Collins-Duffy kernel\n",
    "'''\n",
    "\n",
    "def _deltaP_(tree1, tree2, seq1, seq2, store, lam, mu, p):\n",
    "\n",
    "#     # Enumerate subsequences of length p+1 for each child set\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # generate delta(a,b)\n",
    "        _delta_(tree1, tree2, seq1[-1], seq2[-1], store, lam, mu)\n",
    "        if store[seq1[-1], seq2[-1]] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            runningTot = 0\n",
    "            for i in range(p-1, len(seq1)-1):\n",
    "                for r in range(p-1, len(seq2)-1):\n",
    "                    scaleFactor = pow(lam, len(seq1[:-1])-i+len(seq2[:-1])-r)\n",
    "                    dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-1)\n",
    "                    runningTot += (scaleFactor * dp)\n",
    "            return runningTot\n",
    "\n",
    "def _delta_(tree1, tree2, node1, node2, store, lam, mu):\n",
    "\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "\n",
    "        if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "            if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "                store[node1, node2] = lam\n",
    "            else:\n",
    "                store[node1, node2] = 0\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # establishes p_max\n",
    "            childmin = min(len(tree1[node1]['children']), len(tree2[node2]['children']))\n",
    "            deltaTot = 0\n",
    "            for p in range(1,childmin+1):\n",
    "                # compute delta_p\n",
    "                deltaTot += _deltaP_(tree1, tree2,\n",
    "                                     tree1[node1]['children'],\n",
    "                                     tree2[node2]['children'], store, lam, mu, p)\n",
    "\n",
    "            store[node1, node2] = mu * (pow(lam,2) + deltaTot)\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "def _ptKernel_(tree1, tree2, lam, mu):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _delta_(tree1, tree2, i, j, store, lam, mu)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def MoschittiPT(tree1, tree2, lam, mu, NORMALIZE_FLAG):\n",
    "    raw_score = _ptKernel_(tree1, tree2, lam, mu)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _ptKernel_(tree1, tree1, lam, mu)\n",
    "        t2_score = _ptKernel_(tree2, tree2, lam, mu)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNLPToks(rawSentence):\n",
    "    output = nlp.annotate(rawSentence, properties={'annotators': 'tokenize,ssplit,pos,parse','outputFormat': 'json','timeout':'1000000000'})\n",
    "    output=ast.literal_eval(output)\n",
    "    tokens = output['sentences'][0]['tokens']\n",
    "    parse = output['sentences'][0]['parse'].split(\"\\n\")\n",
    "    return {\n",
    "        'toks':tokens, 'parse':parse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTree(sent): \n",
    "    parsed=getNLPToks(sent)\n",
    "    x=parsed['parse']\n",
    "    s=''\n",
    "    for i in x:\n",
    "        s=s+i\n",
    "    y=Tree.fromstring(s)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoschittiScore(sent1,sent2):\n",
    "    tree_1=tree()\n",
    "    tree_2=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    out2=getNLPToks(sent2)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    generateTree(out2['parse'],tree_2)\n",
    "    flipTree(tree_1)\n",
    "    flipTree(tree_2)\n",
    "    (rscore_st, nscore_st) = MoschittiPT(tree_1, tree_2, 0.8, 1, 1)\n",
    "#     return rscore_st,nscore_st\n",
    "    return nscore_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacardScore(a, b):\n",
    "    # tokens_a = [lemmatizer.lemmatize(token.lower().strip(string.punctuation)) for token in tokenizer.tokenize(a) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # tokens_b = [lemmatizer.lemmatize(token.lower().strip(string.punctuation)) for token in tokenizer.tokenize(b) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    \n",
    "    # tokens_a=[token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # tokens_b=[token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "\n",
    "    # a=sp(a,disable=['parser','ner','textcat','entity'])\n",
    "    # b=sp(b,disable=['parser','ner','textcat','entity'])\n",
    "    tokens_a=[token.lemma_.lower() for token in a if ((token.lemma_.lower() not in stopwords) and \n",
    "                                                          (token.text.lower() not in stopwords))]\n",
    "    tokens_b=[token.lemma_.lower() for token in b if ((token.lemma_.lower() not in stopwords) and \n",
    "                                                          (token.text.lower() not in stopwords))]\n",
    "\n",
    "    if len(set(tokens_a).union(tokens_b))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(tokens_a).intersection(tokens_b)) / float(len(set(tokens_a).union(tokens_b)))\n",
    "    return ratio  \n",
    "\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words=[lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    \n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    words=[token.lemma_.lower() for token in sentence if token.pos_ != 'PUNCT']\n",
    "\n",
    "    # words=[word.lower() for word in words]\n",
    "    # words = sentence.split()\n",
    "    # words = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(sentence) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "'''\n",
    "Returns the average word vector of the paragraph after the removal of stopwords using the pretrained word2vec model\n",
    "'''\n",
    "\n",
    "def avg_feature_vector_without_stopwords(sentence, model, num_features, index2word_set):\n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words = [lemmatizer.lemmatize(token.lower().strip(string.punctuation)) for token in words if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # words = [token.lower().strip(string.punctuation) for token in words if token.lower().strip(string.punctuation) not in stopwords]\n",
    "\n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    words=[token.lemma_.lower() for token in sentence if ((token.lemma_.lower() not in stopwords) and \n",
    "                                                          (token.text.lower() not in stopwords))]\n",
    "\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "\n",
    "'''\n",
    "Returns the average word vector of the nouns in the paragraph using the pretrained word2vec model\n",
    "'''\n",
    "\n",
    "def avg_feature_vector_nouns(sentence, model, num_features, index2word_set):\n",
    "    \n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words=[lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    # words=[word.lower() for word in words]\n",
    "    # words = sentence.split()\n",
    "    # words = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(sentence) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    '''\n",
    "    nouns=[]\n",
    "    for word,pos in nltk.pos_tag(words):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns.append(word.lower().strip(string.punctuation))   \n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    nouns=[token.lemma_.lower() for token in sentence if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN')) ]\n",
    "\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in nouns:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "\n",
    "'''\n",
    "Returns the average word vector of the verbs in the paragraph using the pretrained word2vec model\n",
    "'''\n",
    "\n",
    "def avg_feature_vector_verbs(sentence, model, num_features, index2word_set):\n",
    "    \n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words=[lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    # words=[word.lower() for word in words]\n",
    "    # words = sentence.split()\n",
    "    # words = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(sentence) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    \n",
    "    '''\n",
    "    verbs=[]\n",
    "    for word,pos in nltk.pos_tag(words):\n",
    "        if pos.startswith('VB'):\n",
    "            verbs.append(word.lower().strip(string.punctuation))   \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    verbs=[token.lemma_.lower() for token in sentence if token.pos_ == 'VERB']\n",
    "\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in verbs:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "'''\n",
    "Returns the jaccard index of nouuns in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardNouns(sent1,sent2):\n",
    "    \n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN'))]\n",
    "    # b=sp(sent2)\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN'))]\n",
    "\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "'''\n",
    "Returns the jaccard index of verbs in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardVerbs(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "    \n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if token.pos_ == 'VERB']\n",
    "    # b=sp(sent2)\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if token.pos_ == 'VERB']\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "'''\n",
    "Returns the jaccard index of adjectives in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardAdj(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if token.pos_ == 'ADJ']\n",
    "    # b=sp(sent2,disable=['parser','ner','textcat','entity'])\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if token.pos_ == 'ADJ']\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonProperNouns(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    sent1_tokens=nltk.pos_tag(tokenizer.tokenize(sent1))\n",
    "    sent2_tokens=nltk.pos_tag(tokenizer.tokenize(sent2))\n",
    "    sent1_proper=[word.lower() for (word,tag) in sent1_tokens if tag=='NNP']\n",
    "    sent2_proper=[word.lower() for (word,tag) in sent2_tokens if tag=='NNP']\n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    sent1_proper=[token.lemma_.lower() for token in sent1 if token.pos_ == 'PROPN']\n",
    "    # b=sp(sent2,disable=['parser','ner','textcat','entity'])\n",
    "    sent2_proper=[token.lemma_.lower() for token in sent2 if token.pos_ == 'PROPN']\n",
    "    common=len(set(sent1_proper).intersection(sent2_proper))\n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubsequence(a, b):\n",
    "    a=tokenizer.tokenize(a)\n",
    "    b=tokenizer.tokenize(b)\n",
    "    lengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    # read the substring out from the matrix\n",
    "    result = \"\"\n",
    "    x, y = len(a), len(b)\n",
    "    while x != 0 and y != 0:\n",
    "        if lengths[x][y] == lengths[x-1][y]:\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y-1]:\n",
    "            y -= 1\n",
    "        else:\n",
    "            assert a[x-1] == b[y-1]\n",
    "            result = a[x-1] + \" \" +result\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubsequence_noStopWords(a, b):\n",
    "    a=tokenizer.tokenize(a)\n",
    "    b=tokenizer.tokenize(b)\n",
    "    a=[w.lower() for w in a if w.lower() not in stopwords]\n",
    "    b=[w.lower() for w in b if w.lower() not in stopwords]\n",
    "    lengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    # read the substring out from the matrix\n",
    "    result = \"\"\n",
    "    x, y = len(a), len(b)\n",
    "    while x != 0 and y != 0:\n",
    "        if lengths[x][y] == lengths[x-1][y]:\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y-1]:\n",
    "            y -= 1\n",
    "        else:\n",
    "            assert a[x-1] == b[y-1]\n",
    "            result = a[x-1] + \" \" +result\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(sent1):\n",
    "    tree_1=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    flipTree(tree_1)\n",
    "    return tree_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTokens(tr,sent):\n",
    "    for key in tr.keys():\n",
    "        parse=tr[key]\n",
    "        childrenTok=parse['childrenTok']\n",
    "        if type(childrenTok)==list:\n",
    "            i=0\n",
    "            for word in childrenTok:\n",
    "                if word in sent.split():\n",
    "                    childrenTok[i]='NULLWORD'\n",
    "                i=i+1\n",
    "        if type(childrenTok)==str:\n",
    "            if childrenTok in sent.split():\n",
    "                childrenTok='NULLWORD'\n",
    "                i=i+1\n",
    "        posOrTok=parse['posOrTok']\n",
    "        if posOrTok in sent.split():\n",
    "            parse['posOrTok']='NULLWORD'\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('german')\n",
    "stopwords.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/users2/mehrotsh/Downloads/german.model', binary=True) \n",
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W017074V017.xml',\n",
       " 'W017920V022.xml',\n",
       " 'W017920V024.xml',\n",
       " 'W018646V014.xml',\n",
       " 'W018646V016.xml',\n",
       " 'W018646V021.xml',\n",
       " 'W019281V018.xml',\n",
       " 'W019281V019.xml',\n",
       " 'W019281V020.xml',\n",
       " 'W019281V021.xml',\n",
       " 'W019281V022.xml',\n",
       " 'W019689V014.xml',\n",
       " 'W019689V015.xml',\n",
       " 'W019689V016.xml',\n",
       " 'W019689V021.xml',\n",
       " 'W019689V025.xml',\n",
       " 'W019689V027.xml',\n",
       " 'W019689V032.xml',\n",
       " 'W019689V033.xml',\n",
       " 'W019689V035.xml',\n",
       " 'W019689V037.xml',\n",
       " 'W019689V039.xml',\n",
       " 'W019689V040.xml',\n",
       " 'W019689V041.xml',\n",
       " 'W019689V042.xml',\n",
       " 'W019689V043.xml',\n",
       " 'W019689V044.xml',\n",
       " 'W019689V045.xml',\n",
       " 'W019689V046.xml',\n",
       " 'W019689V048.xml',\n",
       " 'W019689V052.xml',\n",
       " 'W019689V056.xml',\n",
       " 'W019689V057.xml',\n",
       " 'W019689V060.xml',\n",
       " 'W019689V061.xml',\n",
       " 'W020891V017.xml',\n",
       " 'W020891V018.xml',\n",
       " 'W020891V020.xml',\n",
       " 'W020891V024.xml',\n",
       " 'W020891V025.xml',\n",
       " 'W020891V026.xml',\n",
       " 'W020891V027.xml']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing parsing and number of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W019689V014.xml 1\n",
      "W019689V015.xml 2\n",
      "W019689V016.xml 1\n",
      "W019689V048.xml 1\n",
      "W019689V061.xml 1\n",
      "W020891V017.xml 2\n"
     ]
    }
   ],
   "source": [
    "languages=list()\n",
    "for file in dictionaryOfReferences.keys():\n",
    "    for i in range(3):\n",
    "        detector = Detector(dictionaryOfReferences[file][i])\n",
    "        if detector.reliable==False:\n",
    "            print(file,i)\n",
    "        languages.append(detector.language.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es', 'en', 'fr', 'la', 'de', 'it', 'id']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectFileNames=['W019689V015.xml','W020891V017.xml','W017920V024.xml','W019281V019.xml','W019689V060.xml','W020891V027.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running metrics on Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGerman=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=list()\n",
    "orderedTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both not in German\n",
      "nietzsche not german:  W020891V027.xml\n",
      "incorrect. Continuing\n",
      "Both not in German\n",
      "Both not in German\n",
      "incorrect. Continuing\n",
      "nietzsche not german:  W020891V027.xml\n",
      "Both not in German\n",
      "incorrect. Continuing\n",
      "Both not in German\n",
      "nietzsche not german:  W020891V027.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nietzsche not german:  W020891V027.xml\n",
      "nietzsche not german:  W020891V027.xml\n",
      "Both not in German\n",
      "incorrect. Continuing\n",
      "Both not in German\n",
      "incorrect. Continuing\n",
      "incorrect. Continuing\n"
     ]
    }
   ],
   "source": [
    "for key in dictionaryOfReferences.keys():\n",
    "    \n",
    "    if key in incorrectFileNames:\n",
    "        print('incorrect. Continuing')\n",
    "        continue\n",
    "        \n",
    "    ref=dictionaryOfReferences[key]\n",
    "    ni=ref[0]\n",
    "    r=ref[2]\n",
    "    \n",
    "    ni=re.sub('\\[.*?\\]','',ni)\n",
    "    r=re.sub('\\[.*?\\]','',r)\n",
    "    \n",
    "    ni_language_detector=Detector(ni)\n",
    "    ri_language_detector=Detector(r)\n",
    "    \n",
    "    if ni_language_detector.language.code!='de':\n",
    "        print('nietzsche not german: ',file)\n",
    "        continue\n",
    "    \n",
    "    if ni_language_detector.language.code!=ri_language_detector.language.code:\n",
    "        print('Both not in German')\n",
    "        continue\n",
    "    \n",
    "#     print(key)\n",
    "    allGerman.append(key)\n",
    "    \n",
    "    ni_sents=sent_tokenize(ni)\n",
    "    r_sents=sent_tokenize(r)\n",
    "    \n",
    "    spacy_ni=sp(ni)\n",
    "    spacy_r=sp(r)\n",
    "    \n",
    "    s=0\n",
    "    i=0\n",
    "    s_dup=0\n",
    "    i_dup=0\n",
    "    for sent_ni in ni_sents:\n",
    "        tr1=createTree(sent_ni)\n",
    "        tr1_dup=createTree(sent_ni)\n",
    "        tr1_dup=removeTokens(tr1_dup,sent_ni)\n",
    "        for sent_r in r_sents:\n",
    "            try:\n",
    "                tr2=createTree(sent_r)\n",
    "                tr2_dup=createTree(sent_r)\n",
    "                tr2_dup=removeTokens(tr2_dup,sent_r)\n",
    "                nscore_st=MoschittiPT(tr1,tr2,0.8,1,1)[1]\n",
    "                nscore_st_dup=MoschittiPT(tr1_dup,tr2_dup,0.8,1,1)[1]\n",
    "            except RuntimeWarning:\n",
    "                nscore_st=0\n",
    "                nscore_st_dup=0\n",
    "                \n",
    "            s=s+nscore_st\n",
    "            i=i+1\n",
    "            s_dup=s_dup+nscore_st_dup\n",
    "            i_dup=i_dup+1\n",
    "            \n",
    "    avgSyntacticScore=s/i\n",
    "    avgSyntacticScoreWithoutTokens=s_dup/i_dup\n",
    "    \n",
    "    niv=avg_feature_vector(spacy_ni,model,300,index2word_set)\n",
    "    ni_withoutStopWords=avg_feature_vector_without_stopwords(spacy_ni,model,300,index2word_set)\n",
    "    ni_nouns=avg_feature_vector_nouns(spacy_ni,model,300,index2word_set)\n",
    "    ni_verbs=avg_feature_vector_verbs(spacy_ni,model,300,index2word_set)\n",
    "    \n",
    "    \n",
    "    rv=avg_feature_vector(spacy_r,model,300,index2word_set)\n",
    "    r_withoutStopWords=avg_feature_vector_without_stopwords(spacy_r,model,300,index2word_set)\n",
    "    r_nouns=avg_feature_vector_nouns(spacy_r,model,300,index2word_set)\n",
    "    r_verbs=avg_feature_vector_verbs(spacy_r,model,300,index2word_set)\n",
    "\n",
    "    try:\n",
    "        semScore=1 - spatial.distance.cosine(niv, rv)\n",
    "    except RuntimeWarning:\n",
    "        semScore=0\n",
    "    try:\n",
    "        semScore_withoutStop=1 - spatial.distance.cosine(ni_withoutStopWords, r_withoutStopWords)\n",
    "    except RuntimeWarning:\n",
    "        semScore_withoutStop=0\n",
    "    try:\n",
    "        semScore_nouns=1 - spatial.distance.cosine(ni_nouns, r_nouns)\n",
    "    except RuntimeWarning:\n",
    "        semScore_nouns=0\n",
    "    try:\n",
    "        semScore_verbs=1 - spatial.distance.cosine(ni_verbs, r_verbs)   \n",
    "    except RuntimeWarning:\n",
    "        semScore_verbs=0\n",
    "         \n",
    "    jaccard=jacardScore(spacy_ni,spacy_r)\n",
    "    jaccard_nouns=jacardNouns(spacy_ni,spacy_r)\n",
    "    jaccard_verbs=jacardVerbs(spacy_ni,spacy_r)\n",
    "    jaccard_adj=jacardAdj(spacy_ni,spacy_r)\n",
    "    \n",
    "    lcs=longestSubsequence(ni,r)\n",
    "    lcs_stop=longestSubsequence_noStopWords(ni,r)\n",
    "    \n",
    "    lcs_length=len(lcs.split())\n",
    "    lcs_stop_length=len(lcs_stop.split())\n",
    "    \n",
    "    propNouns=commonProperNouns(spacy_ni,spacy_r)\n",
    "    \n",
    "    \n",
    "    tup=(key,avgSyntacticScore,avgSyntacticScoreWithoutTokens,semScore,semScore_withoutStop,semScore_nouns,semScore_verbs,\n",
    "        jaccard,jaccard_nouns,jaccard_verbs,jaccard_adj,lcs,lcs_length,lcs_stop,lcs_stop_length)\n",
    "    \n",
    "    tupOrder=(avgSyntacticScore,semScore,semScore_withoutStop,semScore_nouns,semScore_verbs,\n",
    "              (avgSyntacticScore+semScore_withoutStop)/2,lcs_stop_length,lcs_stop,avgSyntacticScoreWithoutTokens,\n",
    "             propNouns,jaccard_nouns,jaccard_verbs,jaccard_adj)\n",
    "    \n",
    "    scoreTuples.append(tup)\n",
    "    orderedTuples.append(tupOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allGerman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeOutput(newTuples,dictionaryOfReferences,fileName):\n",
    "    f=open(fileName,'w')\n",
    "    i=1\n",
    "    lines=list()\n",
    "    for t in newTuples:\n",
    "        file=t[0]\n",
    "        j=str(i)\n",
    "        \n",
    "        ni=dictionaryOfReferences[file][0]\n",
    "        r=dictionaryOfReferences[file][2]\n",
    "        \n",
    "        ni=re.sub('\\[.*?\\]','',ni)\n",
    "        r=re.sub('\\[.*?\\]','',r)\n",
    "        \n",
    "        lines.append('Pairing: '+j+' '+file)\n",
    "        lines.append('\\n')\n",
    "        lines.append('Nietzsche: \\n\\n'+ni)\n",
    "        lines.append('\\n\\n')\n",
    "        lines.append('Reference: \\n\\n'+r)\n",
    "        lines.append('\\n\\n')\n",
    "        lines.append('Syntactic Score: '+str(t[1]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Syntactic Similarity without tokens: '+str(t[2]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic Score: '+str(t[3]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic Score without stopwords: '+str(t[4]))\n",
    "        lines.append('\\n')\n",
    "        \n",
    "        lines.append('Jaccard: '+str(t[7]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Jaccard of common nouns: '+str(t[8]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Jaccard of common verbs: '+str(t[9]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Jaccard of common adjectives: '+str(t[10]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic similarity nouns: '+str(t[5]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic similarity verbs: '+str(t[6]))\n",
    "        lines.append('\\n\\n')\n",
    "        \n",
    "        lines.append('LCS: '+str(t[11]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Length: '+str(t[12]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('LCS without stopwords: '+str(t[13]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Length: '+str(t[14]))\n",
    "        \n",
    "        lines.append('\\n\\n\\n')    \n",
    "        \n",
    "        i=i+1\n",
    "    f.writelines(lines)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(scoreTuples,dictionaryOfReferences,'../output/nietzsche/metrics7.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickling_on = open('../output/nietzsche/'+'orderedTuples.pickle',\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(orderedTuples, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orderedTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python personal",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
