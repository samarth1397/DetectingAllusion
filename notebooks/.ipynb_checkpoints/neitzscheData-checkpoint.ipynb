{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded data and extracted pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from lxml import etree\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('../../Nachweise/'):\n",
    "    tree = ET.parse('../../Nachweise/'+file)\n",
    "    root=tree.getroot()\n",
    "    children=[]\n",
    "    for child in root:\n",
    "        children.append(child.tag)\n",
    "    if len(children)==4:\n",
    "        fileNames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames=sorted(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryOfReferences=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in fileNames:\n",
    "    text = open('../../Nachweise/'+file).read()\n",
    "    root = etree.fromstring(text)[1]\n",
    "    ni=''.join(root.itertext())\n",
    "    ni=ni.replace('\\t','')\n",
    "    ni=ni.replace('\\n','')\n",
    "    \n",
    "    root = etree.fromstring(text)[2]\n",
    "    cite=''.join(root.itertext())\n",
    "    cite=cite.replace('\\t','')\n",
    "    cite=cite.replace('\\n','')\n",
    "    \n",
    "    root = etree.fromstring(text)[3]\n",
    "    ref=[root.text]\n",
    "    for i in root.iterdescendants():\n",
    "        if i.tag=='fn':\n",
    "            break\n",
    "        if i.text!=None:\n",
    "            ref.append(i.text)\n",
    "        if i.tail!=None:\n",
    "            ref.append(i.tail)\n",
    "    ref=''.join(ref)\n",
    "    ref=ref.replace('\\t','')\n",
    "    ref=ref.replace('\\n','')\n",
    "    \n",
    "    dictionaryOfReferences[file]=[ni,cite,ref]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting metrics from pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import bisect\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import os\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "from nltk.tree import *\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import nltk.stem.snowball\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "from nltk.draw.tree import TreeView\n",
    "from fuzzywuzzy import fuzz\n",
    "from multiprocessing import Pool\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import wordnet \n",
    "from operator import itemgetter\n",
    "# from langdetect import detect\n",
    "from polyglot.detect import Detector\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "public='/home/users2/mehrotsh/scripts/packages/stanford-corenlp-full-2018-02-27/'\n",
    "personal='/home/samarth/stanford-corenlp-full-2018-02-27/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=StanfordCoreNLP('/home/users2/mehrotsh/scripts/packages/stanford-corenlp-full-2018-02-27/',memory='8g',lang='de',timeout=1000000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=spacy.load('de',disable=['parser','ner','textcat','entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(): \n",
    "    return defaultdict(tree)\n",
    "\n",
    "\n",
    "def _leadingSpaces_(target):\n",
    "    return len(target) - len(target.lstrip())\n",
    "\n",
    "def _findParent_(curIndent, parid, treeRef):\n",
    "    tmpid = parid\n",
    "    while (curIndent <= treeRef[tmpid]['indent']):\n",
    "        tmpid = treeRef[tmpid]['parid']\n",
    "    return tmpid\n",
    "\n",
    "\n",
    "def generateTree(rawTokens, treeRef):\n",
    "\n",
    "    # (token\n",
    "    REGEX_OPEN = r\"^\\s*\\(([a-zA-Z0-9_']*)\\s*$\"\n",
    "    # (token (tok1 tok2) (tok3 tok4) .... (tokx toky))\n",
    "    REGEX_COMP = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*((?:[(]([a-zA-Z0-9_;.,?'!]+)\\s*([a-zA-Z0-9_;\\.,?!']+)[)]\\s*)+)\"    \n",
    "    # (, ,) as stand-alone. Used for match() not search()\n",
    "    REGEX_PUNC = r\"^\\s*\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "    # (tok1 tok2) as stand-alone\n",
    "    REGEX_SOLO_PAIR = r\"^\\s*\\(([a-zA-Z0-9_']+)\\s*([a-zA-Z0-9_']+)\\)\"\n",
    "    # (tok1 tok2) used in search()\n",
    "    REGEX_ISOL_IN_COMP = r\"\\(([a-zA-Z0-9_;.,?!']+)\\s*([a-zA-Z0-9_;.,?!']+)\\)\"\n",
    "    # (punc punc) used in search()\n",
    "    REGEX_PUNC_SOLO = r\"\\([,!?.'\\\"]\\s*[,!?.'\\\"]\\)\"\n",
    "   \n",
    "    treeRef[len(treeRef)] = {'curid':0, \n",
    "                             'parid':-1, \n",
    "                             'posOrTok':'ROOT', \n",
    "                             'indent':0,\n",
    "                            'children':[],\n",
    "                            'childrenTok':[]}\n",
    "    ID_CTR = 1\n",
    "    \n",
    "    for tok in rawTokens[1:]:\n",
    "        \n",
    "        curIndent = _leadingSpaces_(tok) \n",
    "        parid = _findParent_(curIndent, ID_CTR-1, treeRef)\n",
    "        \n",
    "        # CHECK FOR COMPOSITE TOKENS\n",
    "        checkChild = re.match(REGEX_COMP, tok)\n",
    "        if (checkChild):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkChild.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            upCTR = ID_CTR\n",
    "            ID_CTR += 1\n",
    "            \n",
    "            subCheck = re.sub(REGEX_PUNC_SOLO,'',checkChild.group(2))\n",
    "            subs = re.findall(REGEX_ISOL_IN_COMP, subCheck) \n",
    "            for ch in subs:\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':upCTR, \n",
    "                                   'posOrTok':ch[0], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "                treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                                   'parid':ID_CTR-1, \n",
    "                                   'posOrTok':ch[1], \n",
    "                                   'indent':curIndent+2,\n",
    "                                  'children':[],\n",
    "                                  'childrenTok':[]}\n",
    "                ID_CTR += 1\n",
    "            continue\n",
    "           \n",
    "\n",
    "            \n",
    "        checkSingle = re.match(REGEX_SOLO_PAIR, tok)\n",
    "        if (checkSingle):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkSingle.group(1), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':ID_CTR-1, \n",
    "                               'posOrTok':checkSingle.group(2), \n",
    "                               'indent':curIndent+2,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        checkPunc = re.match(REGEX_PUNC, tok)\n",
    "        if (checkPunc): # ignore punctuation\n",
    "            continue\n",
    "\n",
    "        checkMatch = re.match(REGEX_OPEN, tok)\n",
    "        if (checkMatch):\n",
    "            treeRef[ID_CTR] = {'curid':ID_CTR, \n",
    "                               'parid':parid, \n",
    "                               'posOrTok':checkMatch.group(1), \n",
    "                               'indent':curIndent,\n",
    "                              'children':[],\n",
    "                              'childrenTok':[]}\n",
    "            ID_CTR += 1\n",
    "            continue\n",
    "\n",
    "    return\n",
    "            \n",
    "\n",
    "def flipTree(treeRef):\n",
    "    # Pass 1 fill in children\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            bisect.insort(treeRef[v['parid']]['children'], k)\n",
    "    # Pass 2 map children to tokens\n",
    "    for k,v in treeRef.items():\n",
    "        if (k > 0):\n",
    "            treeRef[k]['childrenTok'] = [treeRef[ch]['posOrTok'] for ch in treeRef[k]['children']]\n",
    "    treeRef[0]['childrenTok'] = treeRef[1]['posOrTok']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isLeaf_(tree, parentNode):\n",
    "    return (len(tree[parentNode]['children']) == 0)\n",
    "\n",
    "def _isPreterminal_(tree, parentNode):\n",
    "    for idx in tree[parentNode]['children']:\n",
    "        if not _isLeaf_(tree, idx):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel\n",
    "'''\n",
    "\n",
    "def _cdHelper_(tree1, tree2, node1, node2, store, lam, SST_ON):\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "        # same children tokens\n",
    "        if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "            # Check if both nodes are pre-terminal\n",
    "            if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "                store[node1, node2] = lam\n",
    "                return\n",
    "            # Not pre-terminal. Recurse among the children of both token trees.\n",
    "            else:\n",
    "                nChildren = len(tree1[node1]['children'])\n",
    "\n",
    "                runningTotal = None\n",
    "                for idx in range(nChildren):\n",
    "                     # index ->  node_id\n",
    "                    tmp_n1 = tree1[node1]['children'][idx]\n",
    "                    tmp_n2 = tree2[node2]['children'][idx]\n",
    "                    # Recursively run helper\n",
    "                    _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON)\n",
    "                    # Set the initial value for the layer. Else multiplicative product.\n",
    "                    if (runningTotal == None):\n",
    "                        runningTotal = SST_ON + store[tmp_n1, tmp_n2]\n",
    "                    else:\n",
    "                        runningTotal *= (SST_ON + store[tmp_n1, tmp_n2])\n",
    "\n",
    "                store[node1, node2] = lam * runningTotal\n",
    "                return\n",
    "        else:\n",
    "            store[node1, node2] = 0\n",
    "    else: # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "\n",
    "def _cdKernel_(tree1, tree2, lam, SST_ON):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def CollinsDuffy(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON):\n",
    "    raw_score = _cdKernel_(tree1, tree2, lam, SST_ON)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _cdKernel_(tree1, tree1, lam, SST_ON)\n",
    "        t2_score = _cdKernel_(tree2, tree2, lam, SST_ON)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Implementation of the Partial Tree (PT) Kernel from:\n",
    "\"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees\"\n",
    "by Alessandro Moschitti\n",
    "'''\n",
    "\n",
    "'''\n",
    "The delta function is stolen from the Collins-Duffy kernel\n",
    "'''\n",
    "\n",
    "def _deltaP_(tree1, tree2, seq1, seq2, store, lam, mu, p):\n",
    "\n",
    "#     # Enumerate subsequences of length p+1 for each child set\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # generate delta(a,b)\n",
    "        _delta_(tree1, tree2, seq1[-1], seq2[-1], store, lam, mu)\n",
    "        if store[seq1[-1], seq2[-1]] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            runningTot = 0\n",
    "            for i in range(p-1, len(seq1)-1):\n",
    "                for r in range(p-1, len(seq2)-1):\n",
    "                    scaleFactor = pow(lam, len(seq1[:-1])-i+len(seq2[:-1])-r)\n",
    "                    dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-1)\n",
    "                    runningTot += (scaleFactor * dp)\n",
    "            return runningTot\n",
    "\n",
    "def _delta_(tree1, tree2, node1, node2, store, lam, mu):\n",
    "\n",
    "    # No duplicate computations\n",
    "    if store[node1, node2] >= 0:\n",
    "        return\n",
    "\n",
    "    # Leaves yield similarity score by definition\n",
    "    if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)):\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "    # same parent node\n",
    "    if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']:\n",
    "\n",
    "        if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2):\n",
    "            if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']:\n",
    "                store[node1, node2] = lam\n",
    "            else:\n",
    "                store[node1, node2] = 0\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # establishes p_max\n",
    "            childmin = min(len(tree1[node1]['children']), len(tree2[node2]['children']))\n",
    "            deltaTot = 0\n",
    "            for p in range(1,childmin+1):\n",
    "                # compute delta_p\n",
    "                deltaTot += _deltaP_(tree1, tree2,\n",
    "                                     tree1[node1]['children'],\n",
    "                                     tree2[node2]['children'], store, lam, mu, p)\n",
    "\n",
    "            store[node1, node2] = mu * (pow(lam,2) + deltaTot)\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # parent nodes are different\n",
    "        store[node1, node2] = 0\n",
    "        return\n",
    "\n",
    "def _ptKernel_(tree1, tree2, lam, mu):\n",
    "    # Fill the initial state of the store\n",
    "    store = np.empty((len(tree1), len(tree2)))\n",
    "    store.fill(-1)\n",
    "\n",
    "    # O(N^2) to compute the tree dot product\n",
    "    for i in range(len(tree1)):\n",
    "        for j in range(len(tree2)):\n",
    "            _delta_(tree1, tree2, i, j, store, lam, mu)\n",
    "\n",
    "    return store.sum()\n",
    "\n",
    "'''\n",
    "Returns a tuple w/ format: (raw, normalized)\n",
    "If NORMALIZE_FLAG set to False, tuple[1] = -1\n",
    "'''\n",
    "def MoschittiPT(tree1, tree2, lam, mu, NORMALIZE_FLAG):\n",
    "    raw_score = _ptKernel_(tree1, tree2, lam, mu)\n",
    "    if (NORMALIZE_FLAG):\n",
    "        t1_score = _ptKernel_(tree1, tree1, lam, mu)\n",
    "        t2_score = _ptKernel_(tree2, tree2, lam, mu)\n",
    "        return (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))\n",
    "    else:\n",
    "        return (raw_score,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNLPToks(rawSentence):\n",
    "    output = nlp.annotate(rawSentence, properties={'annotators': 'tokenize,ssplit,pos,parse','outputFormat': 'json','timeout':'1000000000'})\n",
    "    output=ast.literal_eval(output)\n",
    "    tokens = output['sentences'][0]['tokens']\n",
    "    parse = output['sentences'][0]['parse'].split(\"\\n\")\n",
    "    return {\n",
    "        'toks':tokens, 'parse':parse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTree(sent): \n",
    "    parsed=getNLPToks(sent)\n",
    "    x=parsed['parse']\n",
    "    s=''\n",
    "    for i in x:\n",
    "        s=s+i\n",
    "    y=Tree.fromstring(s)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoschittiScore(sent1,sent2):\n",
    "    tree_1=tree()\n",
    "    tree_2=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    out2=getNLPToks(sent2)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    generateTree(out2['parse'],tree_2)\n",
    "    flipTree(tree_1)\n",
    "    flipTree(tree_2)\n",
    "    (rscore_st, nscore_st) = MoschittiPT(tree_1, tree_2, 0.8, 1, 1)\n",
    "#     return rscore_st,nscore_st\n",
    "    return nscore_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacardScore(a, b):\n",
    "    # tokens_a = [lemmatizer.lemmatize(token.lower().strip(string.punctuation)) for token in tokenizer.tokenize(a) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # tokens_b = [lemmatizer.lemmatize(token.lower().strip(string.punctuation)) for token in tokenizer.tokenize(b) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    \n",
    "    # tokens_a=[token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # tokens_b=[token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "\n",
    "    # a=sp(a,disable=['parser','ner','textcat','entity'])\n",
    "    # b=sp(b,disable=['parser','ner','textcat','entity'])\n",
    "    tokens_a=[token.lemma_.lower() for token in a if token.lemma_.lower() not in stopwords]\n",
    "    tokens_b=[token.lemma_.lower() for token in b if token.lemma_.lower() not in stopwords]\n",
    "\n",
    "    if len(set(tokens_a).union(tokens_b))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(tokens_a).intersection(tokens_b)) / float(len(set(tokens_a).union(tokens_b)))\n",
    "    return ratio  \n",
    "\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words=[lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    \n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    words=[token.lemma_.lower() for token in sentence if token.pos_ != 'PUNCT']\n",
    "\n",
    "    # words=[word.lower() for word in words]\n",
    "    # words = sentence.split()\n",
    "    # words = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(sentence) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "'''\n",
    "Returns the average word vector of the paragraph after the removal of stopwords using the pretrained word2vec model\n",
    "'''\n",
    "\n",
    "def avg_feature_vector_without_stopwords(sentence, model, num_features, index2word_set):\n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words = [lemmatizer.lemmatize(token.lower().strip(string.punctuation)) for token in words if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # words = [token.lower().strip(string.punctuation) for token in words if token.lower().strip(string.punctuation) not in stopwords]\n",
    "\n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    words=[token.lemma_.lower() for token in sentence if token.lemma_.lower() not in stopwords]\n",
    "\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "\n",
    "'''\n",
    "Returns the average word vector of the nouns in the paragraph using the pretrained word2vec model\n",
    "'''\n",
    "\n",
    "def avg_feature_vector_nouns(sentence, model, num_features, index2word_set):\n",
    "    \n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words=[lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    # words=[word.lower() for word in words]\n",
    "    # words = sentence.split()\n",
    "    # words = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(sentence) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    '''\n",
    "    nouns=[]\n",
    "    for word,pos in nltk.pos_tag(words):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns.append(word.lower().strip(string.punctuation))   \n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    nouns=[token.lemma_.lower() for token in sentence if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN')) ]\n",
    "\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in nouns:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "\n",
    "'''\n",
    "Returns the average word vector of the verbs in the paragraph using the pretrained word2vec model\n",
    "'''\n",
    "\n",
    "def avg_feature_vector_verbs(sentence, model, num_features, index2word_set):\n",
    "    \n",
    "    # English\n",
    "    # words=tokenizer.tokenize(sentence)\n",
    "    # words=[lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    # words=[word.lower() for word in words]\n",
    "    # words = sentence.split()\n",
    "    # words = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(sentence) if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    \n",
    "    '''\n",
    "    verbs=[]\n",
    "    for word,pos in nltk.pos_tag(words):\n",
    "        if pos.startswith('VB'):\n",
    "            verbs.append(word.lower().strip(string.punctuation))   \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sentence,disable=['parser','ner','textcat','entity'])\n",
    "    verbs=[token.lemma_.lower() for token in sentence if token.pos_ == 'VERB']\n",
    "\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in verbs:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec  \n",
    "\n",
    "'''\n",
    "Returns the jaccard index of nouuns in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardNouns(sent1,sent2):\n",
    "    \n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('NN'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN'))]\n",
    "    # b=sp(sent2)\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if ((token.pos_ == 'NOUN') or (token.pos_ == 'PROPN'))]\n",
    "\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "'''\n",
    "Returns the jaccard index of verbs in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardVerbs(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('VB'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "    \n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if token.pos_ == 'VERB']\n",
    "    # b=sp(sent2)\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if token.pos_ == 'VERB']\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "'''\n",
    "Returns the jaccard index of adjectives in the two paragraphs\n",
    "'''\n",
    "\n",
    "def jacardAdj(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    words1=tokenizer.tokenize(sent1)\n",
    "    words2=tokenizer.tokenize(sent2)\n",
    "    words_1=[lemmatizer.lemmatize(word.lower()) for word in words1]\n",
    "    words_2=[lemmatizer.lemmatize(word.lower()) for word in words2]\n",
    "    nouns1=[]\n",
    "    for word,pos in nltk.pos_tag(words_1):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns1.append(word.lower().strip(string.punctuation))\n",
    "    nouns2=[]\n",
    "    for word,pos in nltk.pos_tag(words_2):\n",
    "        if pos.startswith('JJ'):\n",
    "            nouns2.append(word.lower().strip(string.punctuation))\n",
    "    '''\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    nouns1=[token.lemma_.lower() for token in sent1 if token.pos_ == 'ADJ']\n",
    "    # b=sp(sent2,disable=['parser','ner','textcat','entity'])\n",
    "    nouns2=[token.lemma_.lower() for token in sent2 if token.pos_ == 'ADj']\n",
    "\n",
    "    if len(set(nouns1).union(nouns2))==0:\n",
    "        ratio=0\n",
    "    else:\n",
    "        ratio = len(set(nouns1).intersection(nouns2)) / float(len(set(nouns1).union(nouns2)))        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def commonProperNouns(sent1,sent2):\n",
    "\n",
    "    # English\n",
    "    '''\n",
    "    sent1_tokens=nltk.pos_tag(tokenizer.tokenize(sent1))\n",
    "    sent2_tokens=nltk.pos_tag(tokenizer.tokenize(sent2))\n",
    "    sent1_proper=[word.lower() for (word,tag) in sent1_tokens if tag=='NNP']\n",
    "    sent2_proper=[word.lower() for (word,tag) in sent2_tokens if tag=='NNP']\n",
    "    '''\n",
    "\n",
    "    # German\n",
    "    # a=sp(sent1,disable=['parser','ner','textcat','entity'])\n",
    "    sent1_proper=[token.lemma_.lower() for token in sent1 if token.pos_ == 'PROPN']\n",
    "    # b=sp(sent2,disable=['parser','ner','textcat','entity'])\n",
    "    sent2_proper=[token.lemma_.lower() for token in sent2 if token.pos_ == 'PROPN']\n",
    "    common=len(set(sent1_proper).intersection(sent2_proper))\n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubsequence(a, b):\n",
    "    a=tokenizer.tokenize(a)\n",
    "    b=tokenizer.tokenize(b)\n",
    "    lengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    # read the substring out from the matrix\n",
    "    result = \"\"\n",
    "    x, y = len(a), len(b)\n",
    "    while x != 0 and y != 0:\n",
    "        if lengths[x][y] == lengths[x-1][y]:\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y-1]:\n",
    "            y -= 1\n",
    "        else:\n",
    "            assert a[x-1] == b[y-1]\n",
    "            result = a[x-1] + \" \" +result\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubsequence_noStopWords(a, b):\n",
    "    a=tokenizer.tokenize(a)\n",
    "    b=tokenizer.tokenize(b)\n",
    "    a=[w.lower() for w in a if w.lower() not in stopwords]\n",
    "    b=[w.lower() for w in b if w.lower() not in stopwords]\n",
    "    lengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    # read the substring out from the matrix\n",
    "    result = \"\"\n",
    "    x, y = len(a), len(b)\n",
    "    while x != 0 and y != 0:\n",
    "        if lengths[x][y] == lengths[x-1][y]:\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y-1]:\n",
    "            y -= 1\n",
    "        else:\n",
    "            assert a[x-1] == b[y-1]\n",
    "            result = a[x-1] + \" \" +result\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(sent1):\n",
    "    tree_1=tree()\n",
    "    out1=getNLPToks(sent1)\n",
    "    generateTree(out1['parse'],tree_1)\n",
    "    flipTree(tree_1)\n",
    "    return tree_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTokens(tr,sent):\n",
    "    for key in tr.keys():\n",
    "        parse=tr[key]\n",
    "        childrenTok=parse['childrenTok']\n",
    "        if type(childrenTok)==list:\n",
    "            i=0\n",
    "            for word in childrenTok:\n",
    "                if word in sent.split():\n",
    "                    childrenTok[i]='NULLWORD'\n",
    "                i=i+1\n",
    "        if type(childrenTok)==str:\n",
    "            if childrenTok in sent.split():\n",
    "                childrenTok='NULLWORD'\n",
    "                i=i+1\n",
    "        posOrTok=parse['posOrTok']\n",
    "        if posOrTok in sent.split():\n",
    "            parse['posOrTok']='NULLWORD'\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('german')\n",
    "stopwords.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/users2/mehrotsh/Downloads/german.model', binary=True) \n",
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W017074V017.xml',\n",
       " 'W017920V022.xml',\n",
       " 'W017920V024.xml',\n",
       " 'W018646V014.xml',\n",
       " 'W018646V016.xml',\n",
       " 'W018646V021.xml',\n",
       " 'W019281V018.xml',\n",
       " 'W019281V019.xml',\n",
       " 'W019281V020.xml',\n",
       " 'W019281V021.xml',\n",
       " 'W019281V022.xml',\n",
       " 'W019689V014.xml',\n",
       " 'W019689V015.xml',\n",
       " 'W019689V016.xml',\n",
       " 'W019689V021.xml',\n",
       " 'W019689V025.xml',\n",
       " 'W019689V027.xml',\n",
       " 'W019689V032.xml',\n",
       " 'W019689V033.xml',\n",
       " 'W019689V035.xml',\n",
       " 'W019689V037.xml',\n",
       " 'W019689V039.xml',\n",
       " 'W019689V040.xml',\n",
       " 'W019689V041.xml',\n",
       " 'W019689V042.xml',\n",
       " 'W019689V043.xml',\n",
       " 'W019689V044.xml',\n",
       " 'W019689V045.xml',\n",
       " 'W019689V046.xml',\n",
       " 'W019689V048.xml',\n",
       " 'W019689V052.xml',\n",
       " 'W019689V056.xml',\n",
       " 'W019689V057.xml',\n",
       " 'W019689V060.xml',\n",
       " 'W019689V061.xml',\n",
       " 'W020891V017.xml',\n",
       " 'W020891V018.xml',\n",
       " 'W020891V020.xml',\n",
       " 'W020891V024.xml',\n",
       " 'W020891V025.xml',\n",
       " 'W020891V026.xml',\n",
       " 'W020891V027.xml']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing parsing and number of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NN', ['Aeterna']), Tree('NN', ['autem']), Tree('NNS', ['requies'])]), Tree('VP', [Tree('VBP', ['ei']), Tree('NP', [Tree('NP', [Tree('JJ', ['non']), Tree('NN', ['datur'])]), Tree(',', [',']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('NNS', ['nisi'])]), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('NN', ['dilectione'])])])]), Tree('NP', [Tree('FW', ['dei']), Tree('FW', ['qui']), Tree('FW', ['solus']), Tree('FW', ['aeternus']), Tree('FW', ['est.'])])])])]), Tree('.', ['.'])])])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTree(sent_tokenize(dictionaryOfReferences['W017074V017.xml'][2])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(sent_tokenize(dictionaryOfReferences['W017074V017.xml'][0])[0])\n",
    "print(detector.reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(dictionaryOfReferences['W017074V017.xml'][2])\n",
    "print(detector.language.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n",
      "WARNING:polyglot.detect.base:Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W019689V014.xml 1\n",
      "W019689V015.xml 2\n",
      "W019689V016.xml 1\n",
      "W019689V048.xml 1\n",
      "W019689V061.xml 1\n",
      "W020891V017.xml 2\n"
     ]
    }
   ],
   "source": [
    "languages=list()\n",
    "for file in dictionaryOfReferences.keys():\n",
    "    for i in range(3):\n",
    "        detector = Detector(dictionaryOfReferences[file][i])\n",
    "        if detector.reliable==False:\n",
    "            print(file,i)\n",
    "        languages.append(detector.language.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'de', 'es', 'it', 'la', 'fr', 'en']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Penzel, von Preuischen Werbern angeworben, stand als gemeiner Musketier in Knigsberg. Kant hlt ihn vom Katheder zurck, (Kant) indem er ihn fr einen niedertrchtigen Menschen hielt, weil er seinen Soldatenstand so ruhig bisher ertragen habe.',\n",
       " 'Vgl. Immanuel Kants Smtliche Werke, herausgegeben von Karl Rosenkranz und Fried. Wilh. Schubert. Elften Theils. Zweite Abtheilung. Immanuel Kants Biographie. Zum grossen Theil nach handschriftlichen Nachrichten dargestellt von Fried. Wilh. Schubert, Leipzig 1842, S. 80:1Im Nachbericht (KGW III 5/2, S. 1569) wird als Quelle direkt der zitierte Brief Hamanns an Herder angegeben (aus: Johann Georg Hamann, Schriften und Briefe. 4. Theil in 2 Bnde, herausgegeben von Moritz Petri, Hannover 1872). Die weiteren textuellen bereinstimmungen von Nietzsches Notiz mit Rosenkranz Kant-Ausgabe zeigen aber, dass Nietzsche hier letztere in den Hnden hatte. An sich mag dieser Przisierung keine groe Bedeutung zukommen, damit ist aber der m. W. erste eindeutige Nachweis geliefert, dass Nietzsche eine Kant-Ausgabe je vor den Augen gehabt hat. Fr einen berblick ber diese Problematik vgl. Thomas Brobjer, Nietzsches Philosophical Context. An Intellectual Biography, Urbana / Chicago 2008, S. 3640.',\n",
       " 'Im Jahr 1776']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaryOfReferences['W020891V017.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectFileNames=['W019689V015.xml','W020891V017.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running metrics on Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGerman=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTuples=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both not in German\n",
      "nietzsche not german:  W020891V027.xml\n",
      "Both not in German\n",
      "Both not in German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nietzsche not german:  W020891V027.xml\n",
      "Both not in German\n",
      "incorrect. Continuing\n",
      "Both not in German\n",
      "nietzsche not german:  W020891V027.xml\n",
      "nietzsche not german:  W020891V027.xml\n",
      "nietzsche not german:  W020891V027.xml\n",
      "Both not in German\n",
      "Both not in German\n",
      "incorrect. Continuing\n"
     ]
    }
   ],
   "source": [
    "for key in dictionaryOfReferences.keys():\n",
    "    \n",
    "    if key in incorrectFileNames:\n",
    "        print('incorrect. Continuing')\n",
    "        continue\n",
    "        \n",
    "    ref=dictionaryOfReferences[key]\n",
    "    ni=ref[0]\n",
    "    r=ref[2]\n",
    "    \n",
    "    ni=re.sub('\\[.*?\\]','',ni)\n",
    "    r=re.sub('\\[.*?\\]','',r)\n",
    "    \n",
    "    ni_language_detector=Detector(ni)\n",
    "    ri_language_detector=Detector(r)\n",
    "    \n",
    "    if ni_language_detector.language.code!='de':\n",
    "        print('nietzsche not german: ',file)\n",
    "        continue\n",
    "    \n",
    "    if ni_language_detector.language.code!=ri_language_detector.language.code:\n",
    "        print('Both not in German')\n",
    "        continue\n",
    "    \n",
    "#     print(key)\n",
    "    allGerman.append(key)\n",
    "    \n",
    "    ni_sents=sent_tokenize(ni)\n",
    "    r_sents=sent_tokenize(r)\n",
    "    \n",
    "    spacy_ni=sp(ni)\n",
    "    spacy_r=sp(r)\n",
    "    \n",
    "    s=0\n",
    "    i=0\n",
    "    s_dup=0\n",
    "    i_dup=0\n",
    "    for sent_ni in ni_sents:\n",
    "        tr1=createTree(sent_ni)\n",
    "        tr1_dup=createTree(sent_ni)\n",
    "        tr1_dup=removeTokens(tr1_dup,sent_ni)\n",
    "        for sent_r in r_sents:\n",
    "            try:\n",
    "                tr2=createTree(sent_r)\n",
    "                tr2_dup=createTree(sent_r)\n",
    "                tr2_dup=removeTokens(tr2_dup,sent_r)\n",
    "                nscore_st=MoschittiPT(tr1,tr2,0.8,1,1)[1]\n",
    "                nscore_st_dup=MoschittiPT(tr1_dup,tr2_dup,0.8,1,1)[1]\n",
    "            except RuntimeWarning:\n",
    "                nscore_st=0\n",
    "                nscore_st_dup=0\n",
    "                \n",
    "            s=s+nscore_st\n",
    "            i=i+1\n",
    "            s_dup=s_dup+nscore_st_dup\n",
    "            i_dup=i_dup+1\n",
    "            \n",
    "    avgSyntacticScore=s/i\n",
    "    avgSyntacticScoreWithoutTokens=s_dup/i_dup\n",
    "    \n",
    "    niv=avg_feature_vector(spacy_ni,model,300,index2word_set)\n",
    "    ni_withoutStopWords=avg_feature_vector_without_stopwords(spacy_ni,model,300,index2word_set)\n",
    "    ni_nouns=avg_feature_vector_nouns(spacy_ni,model,300,index2word_set)\n",
    "    ni_verbs=avg_feature_vector_verbs(spacy_ni,model,300,index2word_set)\n",
    "    \n",
    "    \n",
    "    rv=avg_feature_vector(spacy_r,model,300,index2word_set)\n",
    "    r_withoutStopWords=avg_feature_vector_without_stopwords(spacy_r,model,300,index2word_set)\n",
    "    r_nouns=avg_feature_vector_nouns(spacy_r,model,300,index2word_set)\n",
    "    r_verbs=avg_feature_vector_verbs(spacy_r,model,300,index2word_set)\n",
    "\n",
    "    try:\n",
    "        semScore=1 - spatial.distance.cosine(niv, rv)\n",
    "    except RuntimeWarning:\n",
    "        semScore=0\n",
    "    try:\n",
    "        semScore_withoutStop=1 - spatial.distance.cosine(ni_withoutStopWords, r_verbs)\n",
    "    except RuntimeWarning:\n",
    "        semScore_withoutStop=0\n",
    "    try:\n",
    "        semScore_nouns=1 - spatial.distance.cosine(ni_nouns, r_verbs)\n",
    "    except RuntimeWarning:\n",
    "        semScore_nouns=0\n",
    "    try:\n",
    "        semScore_verbs=1 - spatial.distance.cosine(ni_verbs, r_verbs)   \n",
    "    except RuntimeWarning:\n",
    "        semScore_verbs=0\n",
    "         \n",
    "    jaccard=jacardScore(spacy_ni,spacy_r)\n",
    "    jaccard_nouns=jacardNouns(spacy_ni,spacy_r)\n",
    "    jaccard_verbs=jacardVerbs(spacy_ni,spacy_r)\n",
    "    jaccard_adj=jacardAdj(spacy_ni,spacy_r)\n",
    "    \n",
    "    lcs=longestSubsequence(ni,r)\n",
    "    lcs_stop=longestSubsequence_noStopWords(ni,r)\n",
    "    \n",
    "    lcs_length=len(lcs.split())\n",
    "    lcs_stop_length=len(lcs_stop.split())\n",
    "    \n",
    "    \n",
    "    tup=(key,avgSyntacticScore,avgSyntacticScoreWithoutTokens,semScore,semScore_withoutStop,semScore_nouns,semScore_verbs,\n",
    "        jaccard,jaccard_nouns,jaccard_verbs,jaccard_adj,lcs,lcs_length,lcs_stop,lcs_stop_length)\n",
    "\n",
    "    scoreTuples.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allGerman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeOutput(newTuples,dictionaryOfReferences,fileName):\n",
    "    f=open(fileName,'w')\n",
    "    i=1\n",
    "    lines=list()\n",
    "    for t in newTuples:\n",
    "        file=t[0]\n",
    "        j=str(i)\n",
    "        \n",
    "        ni=dictionaryOfReferences[file][0]\n",
    "        r=dictionaryOfReferences[file][2]\n",
    "        \n",
    "        ni=re.sub('\\[.*?\\]','',ni)\n",
    "        r=re.sub('\\[.*?\\]','',r)\n",
    "        \n",
    "        lines.append('Pairing: '+j+' '+file)\n",
    "        lines.append('\\n')\n",
    "        lines.append('Nietzsche: \\n\\n'+ni)\n",
    "        lines.append('\\n\\n')\n",
    "        lines.append('Reference: \\n\\n'+r)\n",
    "        lines.append('\\n\\n')\n",
    "        lines.append('Syntactic Score: '+str(t[1]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Syntactic Similarity without tokens: '+str(t[2]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic Score: '+str(t[3]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic Score without stopwords: '+str(t[4]))\n",
    "        lines.append('\\n')\n",
    "        \n",
    "        lines.append('Jaccard: '+str(t[7]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Jaccard of common nouns: '+str(t[8]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Jaccard of common verbs: '+str(t[9]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Jaccard of common adjectives: '+str(t[10]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic similarity nouns: '+str(t[5]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Semantic similarity verbs: '+str(t[6]))\n",
    "        lines.append('\\n\\n')\n",
    "        \n",
    "        lines.append('LCS: '+str(t[11]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Length: '+str(t[12]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('LCS without stopwords: '+str(t[13]))\n",
    "        lines.append('\\n')\n",
    "        lines.append('Length: '+str(t[14]))\n",
    "        \n",
    "        lines.append('\\n\\n\\n')    \n",
    "        \n",
    "        i=i+1\n",
    "    f.writelines(lines)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(scoreTuples,dictionaryOfReferences,'../output/nietzsche/metrics3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python personal",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
